{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = 'data/2024'\n",
    "\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.json'):\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "#         with open(filepath, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#             # print(f\"Contents of {filename}:\")\n",
    "#             print(data['awd_istr_txt'])\n",
    "#             print(data['awd_titl_txt'])\n",
    "#             print(data['abst_narr_txt'])\n",
    "#             print(data['org_long_name'])\n",
    "#             print(data['org_long_name2'])\n",
    "#             print(len(data['pi']))\n",
    "#             print(data['pi'][0]['proj_role_code2'])\n",
    "#             print(data['pi'][0]['pi_full_name'])\n",
    "#             print(data['perf_inst']['perf_inst_name'])\n",
    "#             print(data['pgm_ele'][0]['pgm_ele_long_name'])\n",
    "#             print(data['pgm_ref'][0]['pgm_ref_long_name'])\n",
    "#             # print(json.dumps(data, indent=4))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files in 2022...\n",
      "Reading files in 2024...\n",
      "Reading files in 2023...\n",
      "Reading files in 2021...\n",
      "Reading files in 2020...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_directory = 'data/ranking_data/'\n",
    "records = []\n",
    "\n",
    "def safe_get(data, keys, default=None):\n",
    "    \"\"\"\n",
    "    Safely get a nested key from a dictionary using a list of keys.\n",
    "    Returns default if any key is missing.\n",
    "    \"\"\"\n",
    "    for key in keys:\n",
    "        if isinstance(data, dict) and key in data:\n",
    "            data = data[key]\n",
    "        else:\n",
    "            return default\n",
    "    return data\n",
    "\n",
    "for sub_dir in os.listdir(data_directory):\n",
    "    print(f\"Reading files in {sub_dir}...\")\n",
    "    sub_directory = os.path.join(data_directory, sub_dir)\n",
    "    for filename in os.listdir(sub_directory):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(sub_directory, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Extract award-level context information safely\n",
    "            award_type = data.get(\"awd_istr_txt\")\n",
    "            award_title = data.get(\"awd_titl_txt\")\n",
    "            abstract = data.get(\"abst_narr_txt\")\n",
    "            org_name = data.get(\"org_long_name\")\n",
    "            org_name2 = data.get(\"org_long_name2\")\n",
    "            perf_inst_name = safe_get(data, [\"perf_inst\", \"perf_inst_name\"])\n",
    "            \n",
    "            # Extract program element and reference safely (checking if list exists)\n",
    "            pgm_ele_list = data.get(\"pgm_ele\")\n",
    "            if isinstance(pgm_ele_list, list) and len(pgm_ele_list) > 0:\n",
    "                program_element = pgm_ele_list[0].get(\"pgm_ele_long_name\")\n",
    "            else:\n",
    "                program_element = None\n",
    "\n",
    "            pgm_ref_list = data.get(\"pgm_ref\")\n",
    "            if isinstance(pgm_ref_list, list) and len(pgm_ref_list) > 0:\n",
    "                program_reference = pgm_ref_list[0].get(\"pgm_ref_long_name\")\n",
    "            else:\n",
    "                program_reference = None\n",
    "\n",
    "            # Get investigator information, ensuring it's a list\n",
    "            pi_list = data.get(\"pi\")\n",
    "            if not isinstance(pi_list, list):\n",
    "                continue\n",
    "\n",
    "            # Loop through each investigator in the file\n",
    "            for pi in pi_list:\n",
    "                record = {\n",
    "                    \"award_type\": award_type,\n",
    "                    \"award_title\": award_title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"org_name\": org_name,\n",
    "                    \"org_name2\": org_name2,\n",
    "                    \"perf_inst_name\": perf_inst_name,\n",
    "                    \"program_element\": program_element,\n",
    "                    \"program_reference\": program_reference,\n",
    "                    \"pi_id\": pi.get(\"pi_id\"),\n",
    "                    \"pi_full_name\": pi.get(\"pi_full_name\", \"\").strip() if pi.get(\"pi_full_name\") else None,\n",
    "                    \"role\": pi.get(\"proj_role_code2\", \"\").strip() if pi.get(\"proj_role_code2\") else None,\n",
    "                    \"department\": pi.get(\"pi_dept_name\"),\n",
    "                    \"email\": pi.get(\"pi_email_addr\"),\n",
    "                    \"start_date\": pi.get(\"start_date\")\n",
    "                }\n",
    "                records.append(record)\n",
    "\n",
    "# Create a DataFrame from the records\n",
    "df = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/ranking_data/nsf_dataset.csv')\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "role\n",
       "Principal Investigator              50094\n",
       "Co-Principal Investigator           33018\n",
       "Former Co-Principal Investigator     3276\n",
       "Former Principal Investigator        2658\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.role.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>award_type</th>\n",
       "      <th>award_title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>org_name</th>\n",
       "      <th>org_name2</th>\n",
       "      <th>perf_inst_name</th>\n",
       "      <th>program_element</th>\n",
       "      <th>program_reference</th>\n",
       "      <th>pi_id</th>\n",
       "      <th>pi_full_name</th>\n",
       "      <th>role</th>\n",
       "      <th>department</th>\n",
       "      <th>email</th>\n",
       "      <th>start_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Standard Grant</td>\n",
       "      <td>MRI: Acquisition of a High-Performance Computa...</td>\n",
       "      <td>This project will acquire and deploy a high-pe...</td>\n",
       "      <td>Directorate for Computer and Information Scien...</td>\n",
       "      <td>Office of Advanced Cyberinfrastructure (OAC)</td>\n",
       "      <td>Oklahoma State University</td>\n",
       "      <td>Major Research Instrumentation</td>\n",
       "      <td>WOMEN, MINORITY, DISABLED, NEC</td>\n",
       "      <td>269967889</td>\n",
       "      <td>Terrance   Figy</td>\n",
       "      <td>Co-Principal Investigator</td>\n",
       "      <td>Mathematics, Statistics, and Physics</td>\n",
       "      <td>Terrance.Figy@wichita.edu</td>\n",
       "      <td>2024-08-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Standard Grant</td>\n",
       "      <td>MRI: Acquisition of a High-Performance Computa...</td>\n",
       "      <td>This project will acquire and deploy a high-pe...</td>\n",
       "      <td>Directorate for Computer and Information Scien...</td>\n",
       "      <td>Office of Advanced Cyberinfrastructure (OAC)</td>\n",
       "      <td>Oklahoma State University</td>\n",
       "      <td>Major Research Instrumentation</td>\n",
       "      <td>WOMEN, MINORITY, DISABLED, NEC</td>\n",
       "      <td>269758255</td>\n",
       "      <td>Pratul K Agarwal</td>\n",
       "      <td>Principal Investigator</td>\n",
       "      <td></td>\n",
       "      <td>pratul.agarwal@okstate.edu</td>\n",
       "      <td>2022-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Standard Grant</td>\n",
       "      <td>MRI: Acquisition of a High-Performance Computa...</td>\n",
       "      <td>This project will acquire and deploy a high-pe...</td>\n",
       "      <td>Directorate for Computer and Information Scien...</td>\n",
       "      <td>Office of Advanced Cyberinfrastructure (OAC)</td>\n",
       "      <td>Oklahoma State University</td>\n",
       "      <td>Major Research Instrumentation</td>\n",
       "      <td>WOMEN, MINORITY, DISABLED, NEC</td>\n",
       "      <td>000224099</td>\n",
       "      <td>Mickey   Slimp</td>\n",
       "      <td>Co-Principal Investigator</td>\n",
       "      <td>Department of Chemistry</td>\n",
       "      <td></td>\n",
       "      <td>2024-08-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Standard Grant</td>\n",
       "      <td>MRI: Acquisition of a High-Performance Computa...</td>\n",
       "      <td>This project will acquire and deploy a high-pe...</td>\n",
       "      <td>Directorate for Computer and Information Scien...</td>\n",
       "      <td>Office of Advanced Cyberinfrastructure (OAC)</td>\n",
       "      <td>Oklahoma State University</td>\n",
       "      <td>Major Research Instrumentation</td>\n",
       "      <td>WOMEN, MINORITY, DISABLED, NEC</td>\n",
       "      <td>269666332</td>\n",
       "      <td>William H Hsu</td>\n",
       "      <td>Co-Principal Investigator</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>bhsu@ksu.edu</td>\n",
       "      <td>2022-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Standard Grant</td>\n",
       "      <td>MRI: Acquisition of a High-Performance Computa...</td>\n",
       "      <td>This project will acquire and deploy a high-pe...</td>\n",
       "      <td>Directorate for Computer and Information Scien...</td>\n",
       "      <td>Office of Advanced Cyberinfrastructure (OAC)</td>\n",
       "      <td>Oklahoma State University</td>\n",
       "      <td>Major Research Instrumentation</td>\n",
       "      <td>WOMEN, MINORITY, DISABLED, NEC</td>\n",
       "      <td>270046494</td>\n",
       "      <td>Robert   Fleming</td>\n",
       "      <td>Co-Principal Investigator</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>rofleming@AState.edu</td>\n",
       "      <td>2024-08-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       award_type                                        award_title  \\\n",
       "0  Standard Grant  MRI: Acquisition of a High-Performance Computa...   \n",
       "1  Standard Grant  MRI: Acquisition of a High-Performance Computa...   \n",
       "2  Standard Grant  MRI: Acquisition of a High-Performance Computa...   \n",
       "4  Standard Grant  MRI: Acquisition of a High-Performance Computa...   \n",
       "7  Standard Grant  MRI: Acquisition of a High-Performance Computa...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  This project will acquire and deploy a high-pe...   \n",
       "1  This project will acquire and deploy a high-pe...   \n",
       "2  This project will acquire and deploy a high-pe...   \n",
       "4  This project will acquire and deploy a high-pe...   \n",
       "7  This project will acquire and deploy a high-pe...   \n",
       "\n",
       "                                            org_name  \\\n",
       "0  Directorate for Computer and Information Scien...   \n",
       "1  Directorate for Computer and Information Scien...   \n",
       "2  Directorate for Computer and Information Scien...   \n",
       "4  Directorate for Computer and Information Scien...   \n",
       "7  Directorate for Computer and Information Scien...   \n",
       "\n",
       "                                      org_name2             perf_inst_name  \\\n",
       "0  Office of Advanced Cyberinfrastructure (OAC)  Oklahoma State University   \n",
       "1  Office of Advanced Cyberinfrastructure (OAC)  Oklahoma State University   \n",
       "2  Office of Advanced Cyberinfrastructure (OAC)  Oklahoma State University   \n",
       "4  Office of Advanced Cyberinfrastructure (OAC)  Oklahoma State University   \n",
       "7  Office of Advanced Cyberinfrastructure (OAC)  Oklahoma State University   \n",
       "\n",
       "                  program_element               program_reference      pi_id  \\\n",
       "0  Major Research Instrumentation  WOMEN, MINORITY, DISABLED, NEC  269967889   \n",
       "1  Major Research Instrumentation  WOMEN, MINORITY, DISABLED, NEC  269758255   \n",
       "2  Major Research Instrumentation  WOMEN, MINORITY, DISABLED, NEC  000224099   \n",
       "4  Major Research Instrumentation  WOMEN, MINORITY, DISABLED, NEC  269666332   \n",
       "7  Major Research Instrumentation  WOMEN, MINORITY, DISABLED, NEC  270046494   \n",
       "\n",
       "       pi_full_name                       role  \\\n",
       "0   Terrance   Figy  Co-Principal Investigator   \n",
       "1  Pratul K Agarwal     Principal Investigator   \n",
       "2    Mickey   Slimp  Co-Principal Investigator   \n",
       "4     William H Hsu  Co-Principal Investigator   \n",
       "7  Robert   Fleming  Co-Principal Investigator   \n",
       "\n",
       "                             department                       email  \\\n",
       "0  Mathematics, Statistics, and Physics   Terrance.Figy@wichita.edu   \n",
       "1                                        pratul.agarwal@okstate.edu   \n",
       "2               Department of Chemistry                               \n",
       "4                      Computer Science                bhsu@ksu.edu   \n",
       "7                           Engineering        rofleming@AState.edu   \n",
       "\n",
       "   start_date  \n",
       "0  2024-08-29  \n",
       "1  2022-08-03  \n",
       "2  2024-08-29  \n",
       "4  2022-08-03  \n",
       "7  2024-08-29  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['role'].isin(['Co-Principal Investigator', 'Principal Investigator'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54006, 56701)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.pi_id.unique().tolist()), len(df.pi_full_name.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['award_type', 'award_title', 'abstract', 'org_name', 'org_name2',\n",
       "       'perf_inst_name', 'program_element', 'program_reference', 'pi_id',\n",
       "       'pi_full_name', 'role', 'department', 'email', 'start_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Combine relevant text columns into one (you may adjust columns as needed)\n",
    "text_columns = [\n",
    "    \"award_type\", \"award_title\", \"abstract\", \n",
    "    \"org_name\", \"org_name2\", \"perf_inst_name\", \n",
    "    \"program_element\", \"program_reference\"\n",
    "]\n",
    "df[\"combined_text\"] = df[text_columns].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "# a. Leadership indicator: 1 if role suggests prior leadership (e.g., contains \"Principal Investigator\")\n",
    "df[\"leadership\"] = df[\"role\"].apply(lambda x: 1 if \"Principal Investigator\" in str(x) else 0)\n",
    "\n",
    "# b. Experience in years: use start_date and a reference date (here we use today)\n",
    "df[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors='coerce')\n",
    "reference_date = datetime.now()  # or use a fixed project date\n",
    "df[\"experience_years\"] = (reference_date - df[\"start_date\"]).dt.days / 365.25\n",
    "\n",
    "# Load a pre-trained sentence transformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute embedding for each award's combined text\n",
    "df[\"text_embedding\"] = df[\"combined_text\"].apply(lambda x: embedder.encode(x))\n",
    "\n",
    "# We assume each row has a researcher ID (\"pi_id\"). If a researcher has multiple rows, we aggregate.\n",
    "# For aggregated text, we average the embeddings; for numeric features, we use appropriate aggregation.\n",
    "award_counts = df.groupby(\"pi_id\").size().reset_index(name=\"award_count\")\n",
    "df_grouped = df.groupby(\"pi_id\").agg({\n",
    "    \"experience_years\": \"mean\",       # average experience across awards\n",
    "    \"leadership\": \"max\",              # if they have ever been a PI, mark as leadership\n",
    "    \"text_embedding\": lambda embs: np.mean(np.stack(embs), axis=0)\n",
    "}).reset_index()\n",
    "df_grouped = df_grouped.merge(award_counts, on=\"pi_id\", how=\"left\")\n",
    "\n",
    "# For later scoring, normalize the numeric features (experience and award_count)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_grouped[[\"exp_norm\", \"award_norm\"]] = scaler.fit_transform(df_grouped[[\"experience_years\", \"award_count\"]])\n",
    "\n",
    "def rank_candidates(research_topic, candidate_ids, base_weight=0.5, topic_weight=0.5):\n",
    "    # Compute embedding for the research topic\n",
    "    topic_emb = embedder.encode(research_topic)\n",
    "    \n",
    "    candidate_scores = []\n",
    "    for cid in candidate_ids:\n",
    "        candidate = df_grouped[df_grouped[\"pi_id\"] == cid].iloc[0]\n",
    "        \n",
    "        # Topic relevance score: cosine similarity between candidate's aggregated embedding and the topic\n",
    "        candidate_emb = candidate[\"text_embedding\"]\n",
    "        relevance_score = cosine_similarity([candidate_emb], [topic_emb])[0][0]\n",
    "        \n",
    "        # Base score: a simple weighted sum of normalized features plus a bonus for leadership\n",
    "        # Adjust weights as needed. Here, leadership gets a bonus of 1 if present.\n",
    "        base_score = candidate[\"exp_norm\"] + candidate[\"award_norm\"] + (1 if candidate[\"leadership\"] == 1 else 0)\n",
    "        \n",
    "        # Combined score: weighted combination of base score and topic relevance\n",
    "        combined_score = base_weight * base_score + topic_weight * relevance_score\n",
    "        candidate_scores.append(combined_score)\n",
    "    \n",
    "    candidate_scores = np.array(candidate_scores)\n",
    "    best_index = np.argmax(candidate_scores)\n",
    "    pi_candidate = candidate_ids[best_index]\n",
    "    co_pi_candidates = [cid for i, cid in enumerate(candidate_ids) if i != best_index]\n",
    "    \n",
    "    return pi_candidate, co_pi_candidates, candidate_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pi_id</th>\n",
       "      <th>experience_years</th>\n",
       "      <th>leadership</th>\n",
       "      <th>text_embedding</th>\n",
       "      <th>award_count</th>\n",
       "      <th>exp_norm</th>\n",
       "      <th>award_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000084</td>\n",
       "      <td>1.468857</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0027017621, 0.026450962, 0.04872246, 0.0352...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.262109</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000094</td>\n",
       "      <td>2.904860</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.026076434, -0.012992876, -0.023296108, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526741</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000132</td>\n",
       "      <td>3.676934</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0791296, 0.043611363, -0.075964525, 0.0038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.669021</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000390</td>\n",
       "      <td>2.201232</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.09878656, -0.029403467, 0.008581905, 0.061...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.397074</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000453</td>\n",
       "      <td>2.151951</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.064013764, -0.05129957, -0.045931555, 0.03...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.387992</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pi_id  experience_years  leadership  \\\n",
       "0  000000084          1.468857           1   \n",
       "1  000000094          2.904860           1   \n",
       "2  000000132          3.676934           1   \n",
       "3  000000390          2.201232           1   \n",
       "4  000000453          2.151951           1   \n",
       "\n",
       "                                      text_embedding  award_count  exp_norm  \\\n",
       "0  [0.0027017621, 0.026450962, 0.04872246, 0.0352...            2  0.262109   \n",
       "1  [-0.026076434, -0.012992876, -0.023296108, 0.0...            1  0.526741   \n",
       "2  [-0.0791296, 0.043611363, -0.075964525, 0.0038...            1  0.669021   \n",
       "3  [-0.09878656, -0.029403467, 0.008581905, 0.061...            1  0.397074   \n",
       "4  [-0.064013764, -0.05129957, -0.045931555, 0.03...            2  0.387992   \n",
       "\n",
       "   award_norm  \n",
       "0    0.028571  \n",
       "1    0.000000  \n",
       "2    0.000000  \n",
       "3    0.000000  \n",
       "4    0.028571  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.to_csv('data/ranking_data/final_nsf_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids = ['000025017', '000025762', '000030655']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to get rows where pi_id is in candidate_ids\n",
    "filtered_df = df[df['pi_id'].isin(candidate_ids)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Major Research Instrumentation', 'Cellular Dynamics and Function',\n",
       "       'SBIR Outreach & Tech. Assist', 'CONDENSED MATTER PHYSICS',\n",
       "       'EchemS-Electrochemical Systems', 'Geobiology & Low-Temp Geochem',\n",
       "       'Biology Postdoctoral Research', 'I-Corps',\n",
       "       'Postdoctoral Fellowships', 'Cellular & Biochem Engineering',\n",
       "       'Integrat & Collab Ed & Rsearch', 'IUSE',\n",
       "       'EPMD-ElectrnPhoton&MagnDevices', 'D-ISN-Illicit Supply Networks',\n",
       "       'EDSE-Engineering Design and Systems Engineering',\n",
       "       'Graduate Research Fellowship', 'Physical & Dynamic Meteorology',\n",
       "       'Molecular Biophysics', 'Discovery Research K-12',\n",
       "       'Gravity Theory', 'Robust Intelligence', 'ADVANCE',\n",
       "       'ER2-Ethical & Responsible Res', 'FW-HTF Futr Wrk Hum-Tech Frntr',\n",
       "       'BOCP-Biodiv on Changing Planet', 'Paleoclimate',\n",
       "       'OFFICE OF MULTIDISCIPLINARY AC',\n",
       "       'AMPS-Algorithms for Modern Power Systems',\n",
       "       'EPSCoR RII Track-4: Res Fellow', 'Archaeology',\n",
       "       'INFRASTRUCTURE PROGRAM', 'CSR-Computer Systems Research',\n",
       "       'ALGEBRA,NUMBER THEORY,AND COM',\n",
       "       'SII-Spectrum Innovation Initiative', 'GVF - Global Venture Fund',\n",
       "       'ASCEND - MPS', 'SBIR Phase II', 'Advanced Tech Education Prog',\n",
       "       'EDGE Research', 'Workforce (MSPRF) MathSciPDFellow',\n",
       "       'HCC-Human-Centered Computing', 'Macromolec/Supramolec/Nano',\n",
       "       'Innovation: Bioinformatics', 'Software & Hardware Foundation',\n",
       "       'Marine Geology and Geophysics', 'ESHM-Env, Safety, Health & Med',\n",
       "       'PHYSICAL OCEANOGRAPHY', 'BMMB-Biomech & Mechanobiology', 'AISL',\n",
       "       'CCSS-Comms Circuits & Sens Sys', 'COMPUTATIONAL MATHEMATICS',\n",
       "       'CRII CISE Research Initiation Initiative',\n",
       "       'Robert Noyce Scholarship Pgm',\n",
       "       'Rubin Observatory Operations & Maintenance-NOIRLab',\n",
       "       'LONG TERM ECOLOGICAL RESEARCH', 'FD-Fluid Dynamics',\n",
       "       'TTP-Thermal Transport Process', 'Climate Impact on Human Health',\n",
       "       'SBIR Phase I', 'EngEd-Engineering Education',\n",
       "       'IRES Track I: IRES Sites (IS)', 'PPoSS-PP of Scalable Systems',\n",
       "       'STELLAR ASTRONOMY & ASTROPHYSC', 'Convergence Accelerator Resrch',\n",
       "       'Gravity Exp. & Data Analysis', 'CoPe-Coastlines and People',\n",
       "       'Geomorphology & Land-use Dynam', 'Hydrologic Sciences',\n",
       "       'Human-Envi & Geographical Scis', 'TOPOLOGY',\n",
       "       'Leadership-Class Computing', 'AM-Advanced Manufacturing',\n",
       "       'IUCRC-Indust-Univ Coop Res Ctr', 'Law & Science',\n",
       "       'Algorithmic Foundations', 'STATISTICS',\n",
       "       'Chemical Measurement & Imaging', 'Secure &Trustworthy Cyberspace',\n",
       "       'Science & Technology Studies', 'Climate & Large-Scale Dynamics',\n",
       "       'EPSCoR RII Track-2 FEC', 'EPCN-Energy-Power-Ctrl-Netwrks',\n",
       "       'Special Projects - CNS', None, 'Chemical Catalysis',\n",
       "       'Cyberlearn & Future Learn Tech', 'WORKFORCE IN THE MATHEMAT SCI',\n",
       "       'Ecosystem Science', 'RSCH EXPER FOR UNDERGRAD SITES',\n",
       "       'Population & Community Ecology', 'EPSCoR Co-Funding',\n",
       "       'Instrumentation & Facilities', 'DS -Developmental Sciences',\n",
       "       'PROBABILITY', 'NRI-National Robotics Initiati',\n",
       "       'ANALYSIS PROGRAM', 'Evolutionary Processes',\n",
       "       'APPLIED MATHEMATICS', 'OCE Postdoctoral Fellowships',\n",
       "       'ECR-EDU Core Research', 'CMFP-Chem Mech Funct, and Prop',\n",
       "       'CAREER: FACULTY EARLY CAR DEV', 'ECR:BCSER Capcity STEM Ed Rscr',\n",
       "       'SCIENCE RESOURCES STATISTICS', 'PLASMA PHYSICS',\n",
       "       'Strengthening American Infras.', 'EXTRAGALACTIC ASTRON & COSMOLO',\n",
       "       'HSI-Hispanic Serving Institutions',\n",
       "       'SOLID STATE & MATERIALS CHEMIS', 'Cult Anthro DDRI', 'AGEP',\n",
       "       'Chemical Synthesis', 'Networking Technology and Systems (NeTS)',\n",
       "       'XC-Crosscutting Activities Program',\n",
       "       'STUDIES OF THE EARTHS DEEP INT', 'PFI-Partnrships for Innovation',\n",
       "       'Cultural Anthropology', 'NFE-New Faculty Enhancement',\n",
       "       'CISE Education and Workforce', 'FM-Future Manufacturing',\n",
       "       'ANT Organisms & Ecosystems',\n",
       "       'EPSCoR Research Infrastructure Improvement', 'Geophysics',\n",
       "       'HBCU-EiR - HBCU-Excellence in Research',\n",
       "       'EDA-Eng Diversity Activities', 'ERI-Eng. Research Initiation',\n",
       "       'EPSCoR RII Track-1', 'ECI-Engineering for Civil Infrastructure',\n",
       "       'ELECTRONIC/PHOTONIC MATERIALS', 'GOALI-Grnt Opp Acad Lia wIndus',\n",
       "       'BIOLOGICAL OCEANOGRAPHY',\n",
       "       '(SPRF-FR) SBE Postdoctoral Research Fellowship-Fundamental Research',\n",
       "       'Genetic Mechanisms', 'PHYSICS OF LIVING SYSTEMS', 'Catalysis',\n",
       "       'Info Integration & Informatics', 'Economics',\n",
       "       'FRES-Frontier Rsrch Earth Sci', 'Software Institutes',\n",
       "       'Broadening Participation of Groups Underrepresented in Biology',\n",
       "       'Systematics & Biodiversity Sci', 'POLYMERS', 'Tectonics',\n",
       "       'EnvE-Environmental Engineering', 'Cross-BIO Activities',\n",
       "       'Comm & Information Foundations', 'Ecology of Infectious Diseases',\n",
       "       'Linguistics', 'CYBERCORPS: SCHLAR FOR SER',\n",
       "       'Smart and Connected Health', 'SSA-Special Studies & Analysis',\n",
       "       'MAGNETOSPHERIC PHYSICS', 'S&CC: Smart & Connected Communities',\n",
       "       'Interfacial Engineering Program',\n",
       "       'SWIFT-Spectrum Innov Futr Tech', 'NSF ASTRON & ASTROPHY PSTDC FE',\n",
       "       'Sociology', 'PLANT FUNGAL & MICROB DEV MECH',\n",
       "       'MATHEMATICAL BIOLOGY', 'EPSCoR RII: EPSCoR Research Fellow',\n",
       "       'Symbiosis Infection & Immunity',\n",
       "       'SBP-Science of Broadening Participation',\n",
       "       'WoU-Windows on the Universe: The Era of Multi-messenger Astrophysics',\n",
       "       'Special Projects', 'ORCC-Organism Resp Clim Change',\n",
       "       'ITEST-Inov Tech Exp Stu & Teac', 'FRR-Foundationl Rsrch Robotics',\n",
       "       'Petrology and Geochemistry', 'Physiol Mechs & Biomechanics',\n",
       "       'Engineering of Biomed Systems', 'SPECIAL EMPHASIS PROGRAM',\n",
       "       'CPS-Cyber-Physical Systems', 'Information Technology Researc',\n",
       "       'Special Initiatives', 'CERAMICS', 'OCEANOGRAPHIC INSTRUMENTATION',\n",
       "       'NNA-Navigating the New Arctic', 'Merit Review Systm Spprt Costs',\n",
       "       'Hist Black Colleges and Univ', 'S-STEM-Schlr Sci Tech Eng&Math',\n",
       "       'CYBERINFRASTRUCTURE', 'Biological Anthropology',\n",
       "       'CONDENSED MATTER & MAT THEORY', 'PIPP-Pandemic Prevention',\n",
       "       'EWFD-Eng Workforce Development', 'Security & Preparedness',\n",
       "       'GEOMETRIC ANALYSIS', 'CDS&E-MSS', 'ARCSS-Arctic System Science',\n",
       "       'Science of Science', 'Plant-Biotic Interactions',\n",
       "       'Animal Developmental Mechanism', 'STTR Phase I',\n",
       "       'CSGrad4US-CISE Grad Fellowshps', 'EPSCoR RII: Track-1',\n",
       "       'Atmospheric Chemistry', 'Elem. Particle Physics/Theory',\n",
       "       'T-AP-Trans-Atlantic Platform', 'CSforAll-Computer Sci for All',\n",
       "       'OAC-Advanced Cyberinfrast Core', 'ANS-Arctic Natural Sciences',\n",
       "       'CRCNS-Computation Neuroscience', 'GALACTIC ASTRONOMY PROGRAM',\n",
       "       'EFRI Research Projects', 'AON-Arctic Observing Network',\n",
       "       'Dynamics, Control and System Diagnostics (DCSD)',\n",
       "       'Fairness in Artificial Intelligence',\n",
       "       'SHIPBOARD SCIENTIFIC SUPP EQUI', 'FOUNDATIONS',\n",
       "       'Social Psychology', 'FET-Fndtns of Emerging Tech',\n",
       "       'RES EXP FOR TEACHERS(RET)-SITE', 'Tribal College & Univers Prog',\n",
       "       'ADVANCED TECHNOLOGIES & INSTRM', 'ANT Glaciology',\n",
       "       'Animal Behavior', 'EnvS-Environmtl Sustainability',\n",
       "       'Integrtv Ecological Physiology', 'MSPA-INTERDISCIPLINARY',\n",
       "       'Chemical Oceanography', 'Archaeology DDRI',\n",
       "       'ERC-Eng Research Centers', 'Chemistry of Life Processes',\n",
       "       'AMO Theory/Atomic, Molecular & Optical Physics',\n",
       "       'Antarctic Operations Support', 'POSE',\n",
       "       'SPECIAL PROGRAMS IN ASTRONOMY', 'DDRI Linguistics',\n",
       "       'SOLAR-TERRESTRIAL', 'UBE - Undergraduate Biology Education',\n",
       "       'CFS-Combustion & Fire Systems', 'BioP-Biophotonics',\n",
       "       'LIGO RESEARCH SUPPORT', 'BIOMATERIALS PROGRAM',\n",
       "       'ANT Earth Sciences', 'NUCLEAR THEORY',\n",
       "       'ACCESS-AdvCI Coor Ecos:Ser&Sup', 'Bio Anthro DDRI',\n",
       "       'Human Networks & Data Sci Infr',\n",
       "       'GCR-Growing Convergence Research', 'Capacity: Field Stations',\n",
       "       'Alliances-Minority Participat.', 'Proc Sys, Reac Eng & Mol Therm',\n",
       "       'Cognitive Neuroscience', 'AIB-Acctble Institutions&Behav',\n",
       "       'PMP-Particul&MultiphaseProcess', 'METAL & METALLIC NANOSTRUCTURE',\n",
       "       'CIS-Civil Infrastructure Syst',\n",
       "       'Particle Astrophysics/Underground Physics',\n",
       "       'Nanoscale Interactions Program', 'Campus Cyberinfrastructure',\n",
       "       'EAR-Earth Sciences Research',\n",
       "       'FMitF: Formal Methods in the Field',\n",
       "       'Capacity: Cyberinfrastructure', 'Build and Broaden',\n",
       "       'Sedimentary Geo & Paleobiology',\n",
       "       'DRRG-Disaster Resilience Res Grts',\n",
       "       'DASS-Dsgng Accntble SW Systms', 'AI Research Institutes',\n",
       "       'EDUCATION/HUMAN RESOURCES,OCE', 'AERONOMY',\n",
       "       'GUIRR-Government-University- Industry Research Roundtable',\n",
       "       'Plant Genome Research Project', 'Chem Thry, Mdls & Cmptnl Mthds',\n",
       "       'GOLD-GEO Opps LeadersDiversity',\n",
       "       'Particle Astrophysics & Cosmology/Theory',\n",
       "       'Natural Hazards Engineering Research Infrastructure (NHERI)',\n",
       "       'AMO Experiment/Atomic, Molecular & Optical Physics',\n",
       "       'BRITE-BoostRschIdeasTransEquit', 'FMLoB',\n",
       "       'Mechanics of Materials and Structures (MoMS)',\n",
       "       'OCEAN TECH & INTERDISC COORDIN', 'NIGMS',\n",
       "       'ASSP-Arctic Social Science',\n",
       "       'TRIPODS Transdisciplinary Research in Principles of Data Science',\n",
       "       'Modulation', 'DMR SHORT TERM SUPPORT', 'PAARE', 'Combinatorics',\n",
       "       'Systems and Synthetic Biology', 'PLANETARY ASTRONOMY',\n",
       "       'Evolution of Develp Mechanism',\n",
       "       'HDR-Harnessing the Data Revolution', 'Polar Special Initiatives',\n",
       "       'IceCube Research Support',\n",
       "       'Sustained Availability of Biological Infrastructure',\n",
       "       'PIRE- Prtnrshps Inter Res & Ed', 'OCEANOGRAPHIC TECHNICAL SERVCE',\n",
       "       'CCRI-CISE Cmnty Rsrch Infrstrc', 'STTR Phase II',\n",
       "       'Data Cyberinfrastructure', 'IIS Special Projects',\n",
       "       'HEP-High Energy Physics', 'DYN COUPLED NATURAL-HUMAN',\n",
       "       'ANT Ocean & Atmos Sciences', 'POST DOC/TRAVEL',\n",
       "       'Dimensions of Biodiversity', 'Upper Atmospheric Facilities',\n",
       "       'AIR-Antarctic Infrastructure Recapitalization',\n",
       "       'PPP-Precision Particle Physics', 'Decision, Risk & Mgmt Sci',\n",
       "       'CDS&E', 'OE Operations Engineering',\n",
       "       'MacroSysBIO & NEON-Enabled Science', 'EDUCATION AND WORKFORCE',\n",
       "       'DLI DDRIG-DynLangInfrast DDRIG', 'Methodology, Measuremt & Stats',\n",
       "       'HDBE-Humans, Disasters, and the Built Environment',\n",
       "       'CZO-Critical Zone Obsrvatories', 'NUCLEAR PRECISION MEASUREMENTS',\n",
       "       'EarthCube', 'EDGE Tools', 'SoO-Science Of Organizations',\n",
       "       'BIOSENS-Biosensing', 'CISE MSI Research Expansion',\n",
       "       'Environmental Chemical Science', 'NSF Research Traineeship (NRT)',\n",
       "       'HEGS-DDRI Human-Enviro&Geo Sci',\n",
       "       'DLI-Dyn Language Infrastructure', 'Capacity: Bio Collections',\n",
       "       'Cybersecurity Innovation', 'LHC - ATLAS',\n",
       "       'EDUCATION AND HUMAN RESOURCES', 'SPRF-Broadening Participation',\n",
       "       'NUCLEAR STRUCTURE & REACTIONS', 'CSD-Chem Strcture and Dynamics',\n",
       "       'M3X - Mind, Machine, and Motor Nexus',\n",
       "       'EPSCoR Outreach and Workshops', 'Intl Global Change Res & Coord',\n",
       "       'Activation', 'Particle Astrophysics/Cosmic Phenomena',\n",
       "       'ANT Integrated System Science',\n",
       "       'Nuclear & Hadron Quantum Chromo Dynamics (QCD)',\n",
       "       'Perception, Action & Cognition',\n",
       "       'SRS- Sustainable Regional Systems',\n",
       "       \"LEAP-HI Leading Engineering for America's Prosperity, Health, & Infrastructure\",\n",
       "       'CyberTraining - Training-based Workforce Development for Advanced Cyberinfrastructure',\n",
       "       'Sci of Lrng & Augmented Intel', 'IntgStrat Undst Neurl&Cogn Sys',\n",
       "       'Eddie Bernice Johnson INCLUDES', 'Mid-scale RI - Track 2',\n",
       "       'INTERNATIONAL SUPPORT', 'NextG Network Research',\n",
       "       'Integrative Activities in Physics',\n",
       "       'QIS - Quantum Information Science',\n",
       "       'PHYSICS AT THE INFO FRONTIER', 'RaMP-Res & Mentoring Postbac',\n",
       "       'NSF Engines - Type 1', 'Plant Genome Research Resource',\n",
       "       'BROADENING PARTICIPATION', 'Chemical Instrumentation',\n",
       "       'EDUCATIONAL LINKAGES', 'Disability & Rehab Engineering',\n",
       "       'LET-Life & Enviro Through Time', 'SemiSynBio - Semicon Synth Bio',\n",
       "       'LEAPS-Leading Cultural Change', 'NSF Public Access Initiative',\n",
       "       'AccelNet - Accelerating Research through International Networks',\n",
       "       'Human Networks & Data Sci Res', 'SUBMERSIBLE SUPPORT',\n",
       "       'IRES ASI - Track II: IRES Advance Studies Institutes',\n",
       "       'ASCENT-Address-Chalg-Eng-Teams', 'Cross-Directorate  Activities',\n",
       "       'Centers for Rsch Excell in S&T', 'GEOINFORMATICS',\n",
       "       'EPSCoR RII: Focused EPSCoR Collabs',\n",
       "       'IIASA-Inter Inst App Sys Anlys', 'Innovation: Instrumentation',\n",
       "       'NPGI PostDoc Rsrch Fellowship', 'Gen Admin Cost-AOAM-OIG-NSB',\n",
       "       'UNDERGRADUATE PROGRAMS IN CHEM',\n",
       "       'CiCoE-Cyberinfrastructure Centers of Excellence',\n",
       "       'ARC Rsch Support & Logistics',\n",
       "       'Infrastructure Innovation for Biological Research',\n",
       "       'Organization', 'OCEAN OBSERVATORIES INITIATIVE',\n",
       "       'IAE - LOANS AND GRANTS', 'GLOBAL CHANGE', 'Hiring Assessment LoB',\n",
       "       'FW-HTF-Adv Cogn & Phys Capblty', 'Mid-scale RI - Track 1',\n",
       "       'Space Weather Research', 'ANT Astrophys & Geospace Sci', 'PREM',\n",
       "       'R&D Statistics', 'ADVANCES IN BIO INFORMATICS',\n",
       "       'Resrch Security Stratgy&Policy', 'COVID-19 Research',\n",
       "       'Atmospheric Sci Cluster Prgrm',\n",
       "       'URoL-Understanding the Rules of Life: Predicting Phenotype',\n",
       "       'Unallocated Program Costs', 'Political Science', 'RECODE',\n",
       "       'NAIRR-Nat AI Research Resource', 'Midscale Physics Projects',\n",
       "       'DMREF', 'PHYSICS-BROADEN PARTICIPATION',\n",
       "       'Polar Cyberinfrastructure', 'S&E Indicators', 'GRANTS.GOV',\n",
       "       'LHC - CMS', 'ATW-Alan T Waterman Award', 'ECO-CBET',\n",
       "       'I-Corps Hubs', 'HRLoB', 'PROJECTS',\n",
       "       'Geography and Spatial Sciences', 'MPS DMR INSTRUMENTATION',\n",
       "       'NSF/ENG-UKRI EPSRC Opportunity', 'ANT Instrum & Facilities',\n",
       "       'FARE-Facil for Atmos Res & Ed', 'CHEMISTRY PROJECTS',\n",
       "       'Phase I Ctrs for Chem Innovation (CCI)', 'Special Projects - CCF',\n",
       "       'Geohazards', 'FEDERAL AUDIT CLEARINGHOUSE', 'NSF 2026 Fund',\n",
       "       'iEdison Database', 'ARCHAEOMETRY',\n",
       "       'EPE-Expanding Part. in Engring', \"NSF's OIR Astronomy Lab\",\n",
       "       'COSEMPUP-Committee on Science, Engineering, Medicine, and Public Policy',\n",
       "       'Expeditions in Computing', 'INTERNSHIPS PROGRAM',\n",
       "       'International Research Collab',\n",
       "       'ATD-Algorithms for Threat Detection', 'CFO COUNCIL INTERAGENCY',\n",
       "       'Infrastructure Capacity for Biology', 'OCE SPECIAL PROGRAMS',\n",
       "       'SSIP-Summer Scholars Internship Program',\n",
       "       'OCEAN OBSERVATORY SCI & TECH', 'SBIR/STTR Operations',\n",
       "       'Cross-Agency Priority (CAP) Goal Implementation',\n",
       "       'CESER-Cyberinfrastructure for Emerging Science and Engineering Research',\n",
       "       'ECOSYSTEM STUDIES', 'OTHER MISSION RELATED APPS',\n",
       "       'PREEVENTS - Prediction of and Resilience against Extreme Events',\n",
       "       'CISE Research Resources', 'Behavioral Systems', 'BFE LoB',\n",
       "       'OCE-Ocean Sciences Research', 'Manufacturing Machines & Equip',\n",
       "       'Entrepreneurial Fellows', 'USALS-Ant Logistics Support',\n",
       "       'GEO CI - GEO Cyberinfrastrctre', 'CET Strategic Investments',\n",
       "       'EPIIC-Enbl Part Incr Innov Cap', 'P4CLIMATE',\n",
       "       'NSF Engines - Type 2', 'LEAPS-MPS', 'AI-Safety',\n",
       "       'FuSe-Future of Semiconductors', 'GRANTED',\n",
       "       'Innov TwoYear College STEM Ed', 'Research Sec & Integrity ISAO',\n",
       "       'Innovations in Grad Education', 'Intl Rsrch Exp for Stds (IRES)',\n",
       "       'CRISES-R&I in Sci, Env&Society', 'ExpandQISE',\n",
       "       'ReDDDoT-Resp Des Dev & Dp Tech', 'NDCC-Natl Discvry Cloud Climat',\n",
       "       'CHIRRP: Hzrds & Resilient Plnt', 'ExLENT',\n",
       "       'National STEM Teacher Corps', 'EPSCoR RISE RII',\n",
       "       'APTO-Assess-Predict Tech Outcm', 'Chemistry Infrastructure',\n",
       "       'PHYLOGENETIC SYSTEMATICS',\n",
       "       'QL-The Quantum Leap: Leading the Next Quantum Revolution',\n",
       "       'Partnership Funding from SRC', 'NSF-Ford Foundation Partnrshp',\n",
       "       'TIP-CHIPS KTA-6 Communications', 'OKN-Open Knowledge Networks',\n",
       "       'Big Data Science &Engineering', 'Strengthen the CIP Ecosystem',\n",
       "       'ExpandAI', 'CISE Core: Large Projects',\n",
       "       'NSF-Intel Semiconductr Partnrshp',\n",
       "       'SciSIP-Sci of Sci Innov Policy', 'Global Centers: Track I (IMPL)',\n",
       "       'STEM + Computing (STEM+C) Partnerships Program',\n",
       "       'Global Centers: Track II (DSN)', 'Sustainability in Computing',\n",
       "       'ETAUS-EngTechUnderwaterSci', 'MATERIALS RSCH SCI & ENG CENT',\n",
       "       'National Nanotechnology Coordinated Infrastructure (NNCI)',\n",
       "       'MSI-Manufacturing Systms Integ', 'STCs - 2023 Class',\n",
       "       'ACED-Fab - AdvChipEngDsgn&Fab',\n",
       "       'Program Planning and Policy Development',\n",
       "       'AGS-ATM & Geospace Sciences',\n",
       "       'Cultivating Cultures of Ethical STEM (CCE STEM)',\n",
       "       'AI Institutes - IBM Donation', 'DAT-Democracy AffrmngTchnolgies',\n",
       "       'FIELD STATIONS AND MARINE LABS', 'RCA-Regional Convergence Accel',\n",
       "       'Digitization', 'NEON-Operations & Maintenance',\n",
       "       'S&AS - Smart & Autonomous Systems', 'Gemini-NOIRLab',\n",
       "       'PHYSICS FRONTIER CENTER', 'Accelerating Discovery in Ed',\n",
       "       'CRISP - Critical Resilient Interdependent Infrastructure Systems and Processes',\n",
       "       'Physics Instrumentation', 'ENG NNI Special Studies',\n",
       "       'Innovation: Research Methods', 'AMS FACILITY',\n",
       "       'PFE\\\\RED - Professional Formation of Engineers\\\\Revolutionizing Engineering Departments',\n",
       "       'CHEMISTRY NHMFL', 'LSS-Law And Social Sciences', 'SAGE',\n",
       "       'ANT Coordination & Information', 'NSF-Samsung Partnership',\n",
       "       'SpecEES Spectrum Efficiency, Energy Efficiency and Security',\n",
       "       'SHIP OPERATIONS', 'HBCU-EiR Co-Funding', 'NRAO',\n",
       "       'ART-Accelerating Rsrch Trnsltn', 'Innovative HPC',\n",
       "       'RESEARCH RESOURCES', 'PrecisionHlth Sensors w AI-ML',\n",
       "       'ARCTIC RES & POLICY SUPPORT PR', 'Community Programs',\n",
       "       'MATHEMATICAL SCIENCES RES INST', 'MID-SCALE INSTRUMENTATION',\n",
       "       'Interdiscp Behav&SocSci IBSS', 'ARCTIC RESEARCH PROJECTS',\n",
       "       'International Res Ret Connect', 'Research Coordination Networks',\n",
       "       'Physics Resource Support', 'NSF Eval & Assess',\n",
       "       'RADIO SPECTRUM MANAGEMENT', 'Track 3 INFEWS',\n",
       "       'AI Institute-Intel Donations', 'EARTHSCOPE-SCIENCE UTILIZATION',\n",
       "       'CA-HDR: Convergence Accelerator - Harnessing the Data Revolution',\n",
       "       'STCs - 2021 Class',\n",
       "       'BD Spokes -Big Data Regional Innovation Spokes',\n",
       "       'NEON-Concept & Development',\n",
       "       'CA-FW-HTF: Convergence Accelerator - Future of Work at the Human-Technology Frontier',\n",
       "       'Materials Innovation Platforms', 'NEURAL SYSTEMS CLUSTER',\n",
       "       'ENVIR SOCIAL & BEHAVIOR SCIENC', 'COMPUTATIONAL PHYSICS',\n",
       "       'ICECUBE-OPERATIONS & MAINTENAN',\n",
       "       'Engineering for Natural Hazards (ENH)', 'EARTHSCOPE',\n",
       "       'Materials Eng. & Processing', 'DEL', 'RET SUPPLEMENTS',\n",
       "       'Energy Efficient Computing: from Devices to Architectures (E2CDA)',\n",
       "       'STS-Sci, Tech & Society', 'Collections Postdocs', 'CI REUSE',\n",
       "       'Geography & Spatial Sci-DDRI', 'NANOMANUFACTURING',\n",
       "       'INTEGRATED EARTH SYSTEMS',\n",
       "       'IRES IGE - Track III: New Concepts in International Graduate Experience',\n",
       "       'Track 1 INFEWS', 'I-Corps-Sites', 'GeoPRISMS',\n",
       "       'INTERFAC PROCESSES & THERMODYN',\n",
       "       'SNM - Scalable NanoManufacturing', 'LHC Atlas Construction',\n",
       "       'ESD-Eng & Systems Design', 'EVOLUTIONARY GENETICS',\n",
       "       'Eval & Assessment Capabilites',\n",
       "       'PAESMEM Pres Awrds Excell Mentoring', 'POP & COMMUNITY ECOL PROG',\n",
       "       'LHC CMS Construction', 'CONTROL SYSTEMS', 'CHE CENTERS',\n",
       "       'Biodiversity: Discov &Analysis',\n",
       "       'COVID Impacts on Exisiting Actions',\n",
       "       'Research Infra Cluster Prgrm', 'Science of Learning', 'EARS',\n",
       "       'Accelerating Innovation Rsrch', 'Algorithms in the Field',\n",
       "       'Structural and Architectural Engineering (SAE)',\n",
       "       'Boeing Women in STEM Workforce', 'OCEAN DRILLING PROGRAM',\n",
       "       'AIMS-Antarctic Infrastructure Modernization for Science',\n",
       "       'XD-Extreme Digital', 'MANFG ENTERPRISE SYSTEMS', 'CHRNS',\n",
       "       'Quantum Computing', 'Human Resources Statistics',\n",
       "       'NOIRLab-US ELT Program Office', 'I-Corps-Nodes'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.program_element.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n in df.program_element.unique() if n is not None and 'biom' in str(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.program_element == 'AI-Safety']\n",
    "# 269963391, 269911282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted PI: 269911282\n",
      "Predicted Co-PIs: ['269963391', '000025762', '269878000']\n",
      "Candidate Combined Scores: [0.88965248 1.01012845 0.92949853 0.83147678]\n"
     ]
    }
   ],
   "source": [
    "research_topic = \"STATISTICS\"  # can be any topic, e.g., \"confidential computing\"\n",
    "# candidate_ids = ['000025017', '000025762', '000030655']\n",
    "candidate_ids = ['269963391', '269911282', '000025762', '269878000']\n",
    "pi_candidate, co_pi_candidates, scores = rank_candidates(research_topic, candidate_ids)\n",
    "print(\"Predicted PI:\", pi_candidate)\n",
    "print(\"Predicted Co-PIs:\", co_pi_candidates)\n",
    "print(\"Candidate Combined Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with topic: knowledge graph\n",
      "Predicted PI: 269911282\n",
      "Predicted Co-PIs: ['269963391', '000025762', '269878000']\n",
      "Candidate Combined Scores: [0.97457155 1.11437134 0.88944828 0.79709446]\n",
      "--------------------------------------------------\n",
      "Testing with topic: AI\n",
      "Predicted PI: 269963391\n",
      "Predicted Co-PIs: ['269911282', '000025762', '269878000']\n",
      "Candidate Combined Scores: [1.10800765 1.02881144 0.88151991 0.83231751]\n",
      "--------------------------------------------------\n",
      "Testing with topic: Neuroscience\n",
      "Predicted PI: 269878000\n",
      "Predicted Co-PIs: ['269963391', '269911282', '000025762']\n",
      "Candidate Combined Scores: [0.95705275 0.97001489 0.8349548  1.00054973]\n",
      "--------------------------------------------------\n",
      "Testing with topic: STATISTICS\n",
      "Predicted PI: 269911282\n",
      "Predicted Co-PIs: ['269963391', '000025762', '269878000']\n",
      "Candidate Combined Scores: [0.88965248 1.01012845 0.92949853 0.83147678]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with various topics dynamically.\n",
    "for test_topic in [\"knowledge graph\", \"AI\", \"Neuroscience\", \"STATISTICS\"]:\n",
    "    print(\"Testing with topic:\", test_topic)\n",
    "    pi_candidate, co_pi_candidates, scores = rank_candidates(test_topic, candidate_ids)\n",
    "    print(\"Predicted PI:\", pi_candidate)\n",
    "    print(\"Predicted Co-PIs:\", co_pi_candidates)\n",
    "    print(\"Candidate Combined Scores:\", scores)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m[df\u001b[38;5;241m.\u001b[39mpi_id\u001b[38;5;241m.\u001b[39misin(candidate_ids)][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpi_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleadership\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperience_years\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprogram_element\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[df.pi_id.isin(candidate_ids)][['pi_id', 'role', 'department', 'leadership', 'experience_years', 'program_element']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped[df_grouped.pi_id.isin(candidate_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# ================================\n",
    "# Step 1: Load the Dataset\n",
    "# ================================\n",
    "# Replace \"your_dataset.csv\" with your actual CSV file.\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# ================================\n",
    "# Step 2: Combine Text Fields for Each Award\n",
    "# ================================\n",
    "text_columns = [\n",
    "    \"award_type\", \"award_title\", \"abstract\", \n",
    "    \"org_name\", \"org_name2\", \"perf_inst_name\", \n",
    "    \"program_element\", \"program_reference\"\n",
    "]\n",
    "df[\"combined_text\"] = df[text_columns].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "# ================================\n",
    "# Step 3: Compute Basic Award-Level Features\n",
    "# ================================\n",
    "# (a) Leadership flag: 1 if role indicates prior leadership (e.g., \"Principal Investigator\")\n",
    "# df[\"leadership\"] = df[\"role\"].apply(lambda x: 1 if \"Principal Investigator\" in str(x) else 0)\n",
    "df[\"leadership\"] = df.groupby(\"pi_id\")[\"role\"].transform(lambda x: (x == \"Principal Investigator\").sum())\n",
    "\n",
    "# (b) Experience (in years): using start_date and a reference date (today)\n",
    "df[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors='coerce')\n",
    "reference_date = datetime.now()  # or use a fixed project date\n",
    "df[\"experience_years\"] = (reference_date - df[\"start_date\"]).dt.days / 365.25\n",
    "\n",
    "# ================================\n",
    "# Step 4: Compute Text Embeddings for Each Award\n",
    "# ================================\n",
    "# Load a pre-trained sentence transformer model.\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "df[\"text_embedding\"] = df[\"combined_text\"].apply(lambda x: embedder.encode(x))\n",
    "\n",
    "# ================================\n",
    "# Step 5: Aggregate Features Per Researcher\n",
    "# ================================\n",
    "# For each researcher (identified by \"pi_id\"), we compute:\n",
    "# - Maximum experience (i.e. the highest years value among awards)\n",
    "# - Maximum leadership flag (if they ever held a leadership role)\n",
    "# - Award count\n",
    "# - Aggregated text embedding (average of the embeddings from their awards)\n",
    "award_counts = df.groupby(\"pi_id\").size().reset_index(name=\"award_count\")\n",
    "\n",
    "df_grouped = df.groupby(\"pi_id\").agg({\n",
    "    \"experience_years\": \"max\",\n",
    "    \"leadership\": \"max\",\n",
    "    \"text_embedding\": lambda embs: np.mean(np.stack(embs), axis=0)\n",
    "}).reset_index()\n",
    "\n",
    "df_grouped = df_grouped.merge(award_counts, on=\"pi_id\", how=\"left\")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Normalize Numerical Features\n",
    "# ================================\n",
    "scaler = MinMaxScaler()\n",
    "df_grouped[[\"exp_norm\", \"award_norm\"]] = scaler.fit_transform(\n",
    "    df_grouped[[\"experience_years\", \"award_count\"]]\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 7: Compute a Heuristic Base Score (For Simulation)\n",
    "# ================================\n",
    "# Base score is a simple weighted sum of normalized experience, normalized award count,\n",
    "# and a bonus for leadership.\n",
    "df_grouped[\"base_score\"] = df_grouped[\"exp_norm\"] + df_grouped[\"award_norm\"] + df_grouped[\"leadership\"]\n",
    "\n",
    "# ================================\n",
    "# Step 8: Compute Topic Relevance for a Fixed Training Topic\n",
    "# ================================\n",
    "# For training, we simulate a fixed research topic, e.g. \"knowledge graph\".\n",
    "fixed_topic = \"knowledge graph\"\n",
    "fixed_topic_emb = embedder.encode(fixed_topic)\n",
    "\n",
    "def compute_topic_relevance(emb):\n",
    "    return cosine_similarity([emb], [fixed_topic_emb])[0][0]\n",
    "\n",
    "df_grouped[\"topic_relevance\"] = df_grouped[\"text_embedding\"].apply(compute_topic_relevance)\n",
    "\n",
    "# ================================\n",
    "# Step 9: Construct Training Groups (Simulated Candidate Pools)\n",
    "# ================================\n",
    "# We simulate candidate groups by randomly grouping researchers (e.g., groups of 3)\n",
    "df_grouped = shuffle(df_grouped, random_state=42).reset_index(drop=True)\n",
    "group_size = 3\n",
    "num_groups = len(df_grouped) // group_size\n",
    "group_ids = np.repeat(np.arange(num_groups), group_size)\n",
    "if len(df_grouped) % group_size != 0:\n",
    "    group_ids = np.concatenate([group_ids, np.full(len(df_grouped) % group_size, num_groups)])\n",
    "df_grouped[\"group_id\"] = group_ids\n",
    "\n",
    "# ================================\n",
    "# Step 10: Create Training Labels for Ranking\n",
    "# ================================\n",
    "# Within each group, assign label 1 to the candidate with the highest base_score,\n",
    "# and 0 to the others.\n",
    "df_grouped[\"label\"] = 0\n",
    "for group in df_grouped[\"group_id\"].unique():\n",
    "    group_mask = df_grouped[\"group_id\"] == group\n",
    "    if group_mask.sum() > 0:\n",
    "        best_index = df_grouped.loc[group_mask, \"base_score\"].idxmax()\n",
    "        df_grouped.loc[best_index, \"label\"] = 1\n",
    "\n",
    "# ================================\n",
    "# Step 11: Prepare Data for XGBoost Ranking Model\n",
    "# ================================\n",
    "# Use features: normalized experience, normalized award count, leadership, and topic_relevance.\n",
    "features = [\"exp_norm\", \"award_norm\", \"leadership\", \"topic_relevance\"]\n",
    "X = df_grouped[features].values\n",
    "y = df_grouped[\"label\"].values\n",
    "\n",
    "# XGBoost ranking requires group information: number of candidates in each group.\n",
    "group_data = df_grouped.groupby(\"group_id\").size().tolist()\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "dtrain.set_group(group_data)\n",
    "\n",
    "# ================================\n",
    "# Step 12: Train the XGBoost Ranking Model\n",
    "# ================================\n",
    "params = {\n",
    "    \"objective\": \"rank:pairwise\",\n",
    "    \"eval_metric\": \"ndcg\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"seed\": 42\n",
    "}\n",
    "num_round = 50\n",
    "rank_model = xgb.train(params, dtrain, num_boost_round=num_round)\n",
    "\n",
    "# ================================\n",
    "# Step 13: Define an Inference Function for PI Selection\n",
    "# ================================\n",
    "def predict_pi(candidate_ids, research_topic):\n",
    "    \"\"\"\n",
    "    Given a list of candidate researcher IDs and a research topic,\n",
    "    compute each candidate's features (including topic relevance computed against the provided topic),\n",
    "    predict a ranking score using the trained ranking model, and return candidates ranked in descending order.\n",
    "    \n",
    "    Parameters:\n",
    "      candidate_ids (list): List of candidate pi_id values.\n",
    "      research_topic (str): The research topic (e.g., \"confidential computing\").\n",
    "    \n",
    "    Returns:\n",
    "      ranked_candidates (list): Candidate pi_ids ordered from best (highest score) to worst.\n",
    "      scores (np.array): Predicted ranking scores for each candidate.\n",
    "    \"\"\"\n",
    "    topic_emb = embedder.encode(research_topic)\n",
    "    feature_list = []\n",
    "    \n",
    "    # For each candidate, extract the pre-computed normalized features and recompute topic relevance.\n",
    "    for cid in candidate_ids:\n",
    "        candidate_row = df_grouped[df_grouped[\"pi_id\"] == cid]\n",
    "        if candidate_row.empty:\n",
    "            continue\n",
    "        candidate = candidate_row.iloc[0]\n",
    "        # Recompute topic relevance based on the new research topic\n",
    "        topic_rel = cosine_similarity([candidate[\"text_embedding\"]], [topic_emb])[0][0]\n",
    "        feature_list.append([\n",
    "            candidate[\"exp_norm\"],\n",
    "            candidate[\"award_norm\"],\n",
    "            candidate[\"leadership\"],\n",
    "            topic_rel\n",
    "        ])\n",
    "    if len(feature_list) == 0:\n",
    "        return None, None\n",
    "    X_new = np.array(feature_list)\n",
    "    dtest = xgb.DMatrix(X_new)\n",
    "    preds = rank_model.predict(dtest)\n",
    "    # Rank candidates in descending order (best candidate first)\n",
    "    ranked_indices = np.argsort(-preds)\n",
    "    ranked_candidates = [candidate_ids[i] for i in ranked_indices]\n",
    "    return ranked_candidates, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids = ['000025017', '000025762', '000030655']\n",
    "research_topic = \"STATISTICS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Step 14: Example Usage\n",
    "# ================================\n",
    "# Suppose you have a set of candidate researchers (e.g., from your aggregated dataset)\n",
    "# For demonstration, we randomly pick 4 candidate pi_ids.\n",
    "# candidate_ids = df_grouped[\"pi_id\"].sample(4, random_state=42).tolist()\n",
    "# research_topic = \"confidential computing\"  # This can be any topic of interest\n",
    "\n",
    "ranked_candidates, scores = predict_pi(candidate_ids, research_topic)\n",
    "print(\"Candidate IDs:\", candidate_ids)\n",
    "print(\"Ranked Candidates (best first):\", ranked_candidates)\n",
    "print(\"Ranking Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# ================================\n",
    "# Step 1: Load the Dataset\n",
    "# ================================\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# ================================\n",
    "# Step 2: Combine Text Fields for Each Award\n",
    "# ================================\n",
    "text_columns = [\n",
    "    \"award_type\", \"award_title\", \"abstract\", \n",
    "    \"org_name\", \"org_name2\", \"perf_inst_name\", \n",
    "    \"program_element\", \"program_reference\"\n",
    "]\n",
    "df[\"combined_text\"] = df[text_columns].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "# ================================\n",
    "# Step 3: Compute Award-Level Features\n",
    "# ================================\n",
    "# (a) Leadership flag: 1 if role indicates prior leadership (e.g., \"Principal Investigator\")\n",
    "df[\"leadership\"] = df[\"role\"].apply(lambda x: 1 if \"Principal Investigator\" in str(x) else 0)\n",
    "\n",
    "# (b) Experience (in years): use start_date and a reference date (here, we use current date)\n",
    "df[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors='coerce')\n",
    "reference_date = datetime.now()\n",
    "df[\"experience_years\"] = (reference_date - df[\"start_date\"]).dt.days / 365.25\n",
    "\n",
    "# ================================\n",
    "# Step 4: Compute Text Embeddings for Each Award\n",
    "# ================================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "df[\"text_embedding\"] = df[\"combined_text\"].apply(lambda x: embedder.encode(x))\n",
    "\n",
    "# ================================\n",
    "# Step 5: Aggregate Features Per Researcher\n",
    "# ================================\n",
    "# For each researcher (identified by \"pi_id\"), we compute:\n",
    "# - Maximum experience (i.e. the highest years value among awards)\n",
    "# - Maximum leadership flag (if they ever held a leadership role)\n",
    "# - Award count (number of awards)\n",
    "# - Aggregated text embedding (average of the embeddings from their awards)\n",
    "award_counts = df.groupby(\"pi_id\").size().reset_index(name=\"award_count\")\n",
    "df_grouped = df.groupby(\"pi_id\").agg({\n",
    "    \"experience_years\": \"max\",\n",
    "    \"leadership\": \"max\",\n",
    "    \"text_embedding\": lambda embs: np.mean(np.stack(embs), axis=0)\n",
    "}).reset_index()\n",
    "df_grouped = df_grouped.merge(award_counts, on=\"pi_id\", how=\"left\")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Normalize Numerical Features\n",
    "# ================================\n",
    "scaler = MinMaxScaler()\n",
    "df_grouped[[\"exp_norm\", \"award_norm\"]] = scaler.fit_transform(\n",
    "    df_grouped[[\"experience_years\", \"award_count\"]]\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 7: Compute a Base Score (Independent of Topic)\n",
    "# ================================\n",
    "# For simulation, we use a simple weighted sum of normalized experience and award count,\n",
    "# plus a bonus if the researcher has leadership experience.\n",
    "df_grouped[\"base_score\"] = df_grouped[\"exp_norm\"] + df_grouped[\"award_norm\"] + df_grouped[\"leadership\"]\n",
    "\n",
    "# ================================\n",
    "# Step 8: Simulate Dynamic Topics During Training\n",
    "# ================================\n",
    "# Define a list of possible topics (these can be any research areas)\n",
    "topics_list = [\"knowledge graph\", \"confidential computing\", \"machine learning\", \"data mining\", \"cybersecurity\"]\n",
    "\n",
    "# For simulation, we will form candidate groups and assign each group a random topic.\n",
    "df_grouped = shuffle(df_grouped, random_state=42).reset_index(drop=True)\n",
    "group_size = 3  # or any group size representing candidate pools\n",
    "num_groups = len(df_grouped) // group_size\n",
    "group_ids = np.repeat(np.arange(num_groups), group_size)\n",
    "if len(df_grouped) % group_size != 0:\n",
    "    group_ids = np.concatenate([group_ids, np.full(len(df_grouped) % group_size, num_groups)])\n",
    "df_grouped[\"group_id\"] = group_ids\n",
    "\n",
    "# For each group, assign a random topic and compute dynamic topic relevance for each candidate.\n",
    "# We also simulate the \"true\" ranking label based on a weighted sum of base_score and dynamic topic relevance.\n",
    "base_score_weight = 0.5\n",
    "topic_weight = 0.5\n",
    "\n",
    "# Prepare a column to hold the dynamic topic relevance\n",
    "df_grouped[\"topic_relevance\"] = 0.0\n",
    "df_grouped[\"assigned_topic\"] = \"\"\n",
    "\n",
    "# Process each group\n",
    "for group in df_grouped[\"group_id\"].unique():\n",
    "    group_mask = df_grouped[\"group_id\"] == group\n",
    "    # Assign a random topic from topics_list to this group\n",
    "    dynamic_topic = np.random.choice(topics_list)\n",
    "    df_grouped.loc[group_mask, \"assigned_topic\"] = dynamic_topic\n",
    "    topic_emb = embedder.encode(dynamic_topic)\n",
    "    \n",
    "    # Compute topic relevance for each candidate in the group based on cosine similarity\n",
    "    relevances = []\n",
    "    for idx in df_grouped.loc[group_mask].index:\n",
    "        candidate_emb = df_grouped.at[idx, \"text_embedding\"]\n",
    "        relevance = cosine_similarity([candidate_emb], [topic_emb])[0][0]\n",
    "        relevances.append(relevance)\n",
    "    df_grouped.loc[group_mask, \"topic_relevance\"] = relevances\n",
    "    \n",
    "    # Compute a simulated combined score: weighted sum of base_score and dynamic topic relevance.\n",
    "    combined_scores = base_score_weight * df_grouped.loc[group_mask, \"base_score\"] + topic_weight * df_grouped.loc[group_mask, \"topic_relevance\"]\n",
    "    # Assign label 1 to the candidate with the highest combined score; 0 for others.\n",
    "    best_idx = combined_scores.idxmax()\n",
    "    df_grouped.loc[group_mask, \"label\"] = 0  # initialize\n",
    "    df_grouped.at[best_idx, \"label\"] = 1\n",
    "\n",
    "# ================================\n",
    "# Step 9: Prepare Data for XGBoost Ranking Model\n",
    "# ================================\n",
    "# Our training features include: normalized experience, normalized award count, leadership flag, and dynamic topic_relevance.\n",
    "features = [\"exp_norm\", \"award_norm\", \"leadership\", \"topic_relevance\"]\n",
    "X = df_grouped[features].values\n",
    "y = df_grouped[\"label\"].values\n",
    "\n",
    "# XGBoost ranking requires group information (number of candidates per group)\n",
    "group_data = df_grouped.groupby(\"group_id\").size().tolist()\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "dtrain.set_group(group_data)\n",
    "\n",
    "# ================================\n",
    "# Step 10: Train the XGBoost Ranking Model\n",
    "# ================================\n",
    "params = {\n",
    "    \"objective\": \"rank:pairwise\",\n",
    "    \"eval_metric\": \"ndcg\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"seed\": 42\n",
    "}\n",
    "num_round = 50\n",
    "rank_model = xgb.train(params, dtrain, num_boost_round=num_round)\n",
    "\n",
    "# ================================\n",
    "# Step 11: Define an Inference Function for Dynamic Topic Ranking\n",
    "# ================================\n",
    "def predict_pi(candidate_ids, research_topic):\n",
    "    \"\"\"\n",
    "    Given a list of candidate researcher IDs and a research topic,\n",
    "    compute each candidate's dynamic topic relevance and then predict ranking scores\n",
    "    using the trained ranking model. Returns candidates ranked in descending order.\n",
    "    \n",
    "    Parameters:\n",
    "      candidate_ids (list): List of candidate pi_id values.\n",
    "      research_topic (str): The research topic (e.g., \"confidential computing\").\n",
    "    \n",
    "    Returns:\n",
    "      ranked_candidates (list): Candidate pi_ids ordered from best (highest score) to worst.\n",
    "      scores (np.array): Predicted ranking scores for each candidate.\n",
    "    \"\"\"\n",
    "    topic_emb = embedder.encode(research_topic)\n",
    "    feature_list = []\n",
    "    \n",
    "    # For each candidate, extract pre-computed normalized features and recompute topic relevance using the new topic.\n",
    "    for cid in candidate_ids:\n",
    "        candidate_row = df_grouped[df_grouped[\"pi_id\"] == cid]\n",
    "        if candidate_row.empty:\n",
    "            continue\n",
    "        candidate = candidate_row.iloc[0]\n",
    "        # Recompute topic relevance for the provided research_topic\n",
    "        topic_rel = cosine_similarity([candidate[\"text_embedding\"]], [topic_emb])[0][0]\n",
    "        feature_list.append([\n",
    "            candidate[\"exp_norm\"],\n",
    "            candidate[\"award_norm\"],\n",
    "            candidate[\"leadership\"],\n",
    "            topic_rel\n",
    "        ])\n",
    "    if len(feature_list) == 0:\n",
    "        return None, None\n",
    "    X_new = np.array(feature_list)\n",
    "    dtest = xgb.DMatrix(X_new)\n",
    "    preds = rank_model.predict(dtest)\n",
    "    ranked_indices = np.argsort(-preds)  # descending order\n",
    "    ranked_candidates = [candidate_ids[i] for i in ranked_indices]\n",
    "    return ranked_candidates, preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_ids = ['000025017', '000025762', '000030655']\n",
    "research_topic = \"STATISTICS\"\n",
    "\n",
    "ranked_candidates, scores = predict_pi(candidate_ids, research_topic)\n",
    "print(\"Candidate IDs:\", candidate_ids)\n",
    "print(\"Ranked Candidates (best first):\", ranked_candidates)\n",
    "print(\"Ranking Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Step 12: Example Usage\n",
    "# ================================\n",
    "# Here we simulate a few candidate groups with dynamic topics.\n",
    "# For example, we pick candidate lists and provide a research topic.\n",
    "candidate_ids_example = df_grouped[\"pi_id\"].sample(4, random_state=42).tolist()\n",
    "\n",
    "# Now test with various topics dynamically:\n",
    "for test_topic in [\"knowledge graph\", \"confidential computing\", \"data mining\", 'STATISTICS']:\n",
    "    ranked_candidates, scores = predict_pi(candidate_ids_example, test_topic)\n",
    "    print(\"Research Topic:\", test_topic)\n",
    "    print(\"Candidate IDs:\", candidate_ids_example)\n",
    "    print(\"Ranked Candidates (best first):\", ranked_candidates)\n",
    "    print(\"Ranking Scores:\", scores)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pi_id.isin(candidate_ids_example)][['pi_id', 'role', 'department', 'leadership', 'experience_years']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Step 2: Combine Text Fields for Each Award\n",
    "# ================================\n",
    "text_columns = [\n",
    "    \"award_type\", \"award_title\", \"abstract\", \n",
    "    \"org_name\", \"org_name2\", \"perf_inst_name\", \n",
    "    \"program_element\", \"program_reference\"\n",
    "]\n",
    "df[\"combined_text\"] = df[text_columns].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "# ================================\n",
    "# Step 3: Compute Award-Level Features\n",
    "# ================================\n",
    "# Leadership flag: 1 if role suggests prior leadership (e.g., \"Principal Investigator\")\n",
    "# df[\"leadership\"] = df[\"role\"].apply(lambda x: 1 if \"Principal Investigator\" in str(x) else 0)\n",
    "df[\"leadership\"] = df.groupby(\"pi_id\")[\"role\"].transform(lambda x: (x == \"Principal Investigator\").sum())\n",
    "\n",
    "# Experience (in years) using start_date and current date\n",
    "df[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors='coerce')\n",
    "reference_date = datetime.now()\n",
    "df[\"experience_years\"] = (reference_date - df[\"start_date\"]).dt.days / 365.25\n",
    "\n",
    "# ================================\n",
    "# Step 4: Compute Text Embeddings for Each Award\n",
    "# ================================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "df[\"text_embedding\"] = df[\"combined_text\"].apply(lambda x: embedder.encode(x))\n",
    "\n",
    "# ================================\n",
    "# Step 5: Aggregate Features Per Researcher\n",
    "# ================================\n",
    "# For each researcher (identified by \"pi_id\"), aggregate features:\n",
    "award_counts = df.groupby(\"pi_id\").size().reset_index(name=\"award_count\")\n",
    "df_grouped = df.groupby(\"pi_id\").agg({\n",
    "    \"experience_years\": \"max\",   # best (largest) experience across awards\n",
    "    \"leadership\": \"max\",         # if ever a PI, flag as 1\n",
    "    \"text_embedding\": lambda embs: np.mean(np.stack(embs), axis=0)\n",
    "}).reset_index()\n",
    "df_grouped = df_grouped.merge(award_counts, on=\"pi_id\", how=\"left\")\n",
    "\n",
    "# Also bring in department information (assuming each researcher has a unique department)\n",
    "# If a researcher has multiple departments, choose one or aggregate appropriately.\n",
    "dept_info = df.groupby(\"pi_id\")[\"department\"].first().reset_index()\n",
    "df_grouped = df_grouped.merge(dept_info, on=\"pi_id\", how=\"left\")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Normalize Numerical Features\n",
    "# ================================\n",
    "scaler = MinMaxScaler()\n",
    "df_grouped[[\"exp_norm\", \"award_norm\"]] = scaler.fit_transform(\n",
    "    df_grouped[[\"experience_years\", \"award_count\"]]\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 7: Compute a Base Score (Independent of Topic)\n",
    "# ================================\n",
    "# Simple weighted sum: experience, award count, and leadership bonus.\n",
    "df_grouped[\"base_score\"] = df_grouped[\"exp_norm\"] + df_grouped[\"award_norm\"] + df_grouped[\"leadership\"]\n",
    "\n",
    "# ================================\n",
    "# Step 8: Incorporate Department Information\n",
    "# ================================\n",
    "# One-hot encode the 'department' column.\n",
    "dept_dummies = pd.get_dummies(df_grouped[\"department\"], prefix=\"dept\")\n",
    "df_grouped = pd.concat([df_grouped, dept_dummies], axis=1)\n",
    "# Get a list of department feature names.\n",
    "department_features = list(dept_dummies.columns)\n",
    "\n",
    "# ================================\n",
    "# Step 9: Simulate Dynamic Topics During Training\n",
    "# ================================\n",
    "# Define a list of topics (these are example research areas).\n",
    "topics_list = [\"knowledge graph\", \"confidential computing\", \"machine learning\", \"data mining\", \"cybersecurity\", \"statistics\"]\n",
    "df_grouped = shuffle(df_grouped, random_state=42).reset_index(drop=True)\n",
    "group_size = 3  # candidate pool size\n",
    "num_groups = len(df_grouped) // group_size\n",
    "group_ids = np.repeat(np.arange(num_groups), group_size)\n",
    "if len(df_grouped) % group_size != 0:\n",
    "    group_ids = np.concatenate([group_ids, np.full(len(df_grouped) % group_size, num_groups)])\n",
    "df_grouped[\"group_id\"] = group_ids\n",
    "\n",
    "# Set weights for simulation\n",
    "base_score_weight = 0.5\n",
    "topic_weight = 0.5\n",
    "\n",
    "# Initialize topic relevance and assigned topic columns\n",
    "df_grouped[\"topic_relevance\"] = 0.0\n",
    "df_grouped[\"assigned_topic\"] = \"\"\n",
    "\n",
    "for group in df_grouped[\"group_id\"].unique():\n",
    "    group_mask = df_grouped[\"group_id\"] == group\n",
    "    dynamic_topic = np.random.choice(topics_list)\n",
    "    df_grouped.loc[group_mask, \"assigned_topic\"] = dynamic_topic\n",
    "    topic_emb = embedder.encode(dynamic_topic)\n",
    "    \n",
    "    relevances = []\n",
    "    for idx in df_grouped.loc[group_mask].index:\n",
    "        candidate_emb = df_grouped.at[idx, \"text_embedding\"]\n",
    "        relevance = cosine_similarity([candidate_emb], [topic_emb])[0][0]\n",
    "        relevances.append(relevance)\n",
    "    df_grouped.loc[group_mask, \"topic_relevance\"] = relevances\n",
    "    \n",
    "    # Combined score now includes the dynamic topic relevance.\n",
    "    combined_scores = base_score_weight * df_grouped.loc[group_mask, \"base_score\"] + topic_weight * df_grouped.loc[group_mask, \"topic_relevance\"]\n",
    "    best_idx = combined_scores.idxmax()\n",
    "    df_grouped.loc[group_mask, \"label\"] = 0\n",
    "    df_grouped.at[best_idx, \"label\"] = 1\n",
    "\n",
    "# ================================\n",
    "# Step 10: Prepare Data for XGBoost Ranking Model\n",
    "# ================================\n",
    "# Define feature list, now including department one-hot features.\n",
    "features = [\"exp_norm\", \"award_norm\", \"leadership\", \"topic_relevance\"] + department_features\n",
    "X = df_grouped[features].values\n",
    "y = df_grouped[\"label\"].values\n",
    "\n",
    "# Group information for ranking.\n",
    "group_data = df_grouped.groupby(\"group_id\").size().tolist()\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "dtrain.set_group(group_data)\n",
    "\n",
    "# ================================\n",
    "# Step 11: Train the XGBoost Ranking Model\n",
    "# ================================\n",
    "params = {\n",
    "    \"objective\": \"rank:pairwise\",\n",
    "    \"eval_metric\": \"ndcg\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"seed\": 42\n",
    "}\n",
    "num_round = 50\n",
    "rank_model = xgb.train(params, dtrain, num_boost_round=num_round)\n",
    "\n",
    "# ================================\n",
    "# Step 12: Inference Function for Dynamic Topic Ranking\n",
    "# ================================\n",
    "def predict_pi(candidate_ids, research_topic):\n",
    "    \"\"\"\n",
    "    Given a list of candidate researcher IDs and a research topic,\n",
    "    compute each candidate's dynamic topic relevance and additional department features,\n",
    "    then predict ranking scores using the trained ranking model.\n",
    "    \n",
    "    Returns:\n",
    "      ranked_candidates (list): Candidate pi_ids ordered from best to worst.\n",
    "      scores (np.array): Ranking scores for each candidate.\n",
    "    \"\"\"\n",
    "    topic_emb = embedder.encode(research_topic)\n",
    "    feature_list = []\n",
    "    \n",
    "    # For each candidate, recompute topic relevance with the new research_topic.\n",
    "    for cid in candidate_ids:\n",
    "        candidate_row = df_grouped[df_grouped[\"pi_id\"] == cid]\n",
    "        if candidate_row.empty:\n",
    "            continue\n",
    "        candidate = candidate_row.iloc[0]\n",
    "        topic_rel = cosine_similarity([candidate[\"text_embedding\"]], [topic_emb])[0][0]\n",
    "        # Build the feature vector: use normalized features and department dummy values.\n",
    "        dept_features = candidate[department_features].values.tolist()\n",
    "        features_vector = [\n",
    "            candidate[\"exp_norm\"],\n",
    "            candidate[\"award_norm\"],\n",
    "            candidate[\"leadership\"],\n",
    "            topic_rel\n",
    "        ] + dept_features\n",
    "        feature_list.append(features_vector)\n",
    "    if len(feature_list) == 0:\n",
    "        return None, None\n",
    "    X_new = np.array(feature_list)\n",
    "    dtest = xgb.DMatrix(X_new)\n",
    "    preds = rank_model.predict(dtest)\n",
    "    ranked_indices = np.argsort(-preds)  # sort descending by score\n",
    "    ranked_candidates = [candidate_ids[i] for i in ranked_indices]\n",
    "    return ranked_candidates, preds\n",
    "\n",
    "# ================================\n",
    "# Step 13: Example Usage with Dynamic Topics\n",
    "# ================================\n",
    "# Sample a set of candidate researcher IDs.\n",
    "candidate_ids_example = df_grouped[\"pi_id\"].sample(4, random_state=42).tolist()\n",
    "\n",
    "# Test with various topics dynamically.\n",
    "for test_topic in [\"knowledge graph\", \"confidential computing\", \"data mining\", \"STATISTICS\"]:\n",
    "    ranked_candidates, scores = predict_pi(candidate_ids_example, test_topic)\n",
    "    print(\"Research Topic:\", test_topic)\n",
    "    print(\"Candidate IDs:\", candidate_ids_example)\n",
    "    print(\"Ranked Candidates (best first):\", ranked_candidates)\n",
    "    print(\"Ranking Scores:\", scores)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pi_id.isin(candidate_ids_example)][['pi_id', 'role', 'department', 'leadership', 'experience_years']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topic = \"STATISTICS\"  # can be any topic, e.g., \"confidential computing\"\n",
    "candidate_ids = ['000025017', '000025762', '000030655']\n",
    "ranked_candidates, scores = predict_pi(candidate_ids, research_topic)\n",
    "print(\"Research Topic:\", test_topic)\n",
    "print(\"Candidate IDs:\", candidate_ids_example)\n",
    "print(\"Ranked Candidates (best first):\", ranked_candidates)\n",
    "print(\"Ranking Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.pi_id.isin(candidate_ids)][['pi_id', 'role', 'department', 'leadership', 'experience_years']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
