{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pprint\n",
    "import json\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open('results/QA_Movie_Gemini.json', 'r') as f:\n",
    "# with open('Q_A_LLM_Gemini.json', 'r') as f:\n",
    "    evaluation_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31842/3666243940.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Accumulate results\n",
    "rouge_scores = []\n",
    "bleu_scores = []\n",
    "cosine_similarities = []\n",
    "\n",
    "for data in evaluation_data:\n",
    "    question = data[\"question\"]\n",
    "    expected_answer = data[\"answer\"]\n",
    "    generated_answer = data[\"LLM\"]  # Use the generated answer from the JSON data\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    rouge_result = rouge.compute(predictions=[generated_answer], references=[expected_answer])\n",
    "    bleu_result = bleu.compute(predictions=[generated_answer], references=[[expected_answer]])\n",
    "\n",
    "    # Compute cosine similarity between embeddings\n",
    "    reference_embedding = embed_model.embed_query(expected_answer)\n",
    "    generated_embedding = embed_model.embed_query(generated_answer)\n",
    "    cosine_sim = cosine_similarity([reference_embedding], [generated_embedding])[0][0]\n",
    "\n",
    "    # Store the results\n",
    "    rouge_scores.append(rouge_result)\n",
    "    bleu_scores.append(bleu_result)\n",
    "    cosine_similarities.append(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Generated Answer</th>\n",
       "      <th>Expected Answer</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>ROUGE-Lsum</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_2</th>\n",
       "      <th>Precision_3</th>\n",
       "      <th>Precision_4</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the genres of 'Star Wars: Episode IV ...</td>\n",
       "      <td>The genres of 'Star Wars: Episode IV - A New H...</td>\n",
       "      <td>The genres of 'Star Wars: Episode IV - A New H...</td>\n",
       "      <td>0.986958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.676192</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many users rated 'Jungle Book, The'?</td>\n",
       "      <td>1 user rated 'Jungle Book, The'</td>\n",
       "      <td>1 users rated 'Jungle Book, The'.</td>\n",
       "      <td>0.975744</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866878</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like the movie 'Dumb &amp; Dumber (Dumb and Dumb...</td>\n",
       "      <td>Based on the genres of the movie 'Dumb &amp; Dumbe...</td>\n",
       "      <td>Based on your preference for 'Dumb &amp; Dumber (D...</td>\n",
       "      <td>0.982890</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.762731</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.849366</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>49</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the ratings for 'Taxi Driver'?</td>\n",
       "      <td>User 1 rated 'Taxi Driver' with a rating of 5.0</td>\n",
       "      <td>The ratings for 'Taxi Driver' are 5.0.</td>\n",
       "      <td>0.881614</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Find all movies released in 1993.</td>\n",
       "      <td>The movies released in 1993 are: Three Colors:...</td>\n",
       "      <td>The movies released in 1993 are 'Three Colors:...</td>\n",
       "      <td>0.941492</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.280099</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.908324</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What are the genres of 'Star Wars: Episode IV ...   \n",
       "1           How many users rated 'Jungle Book, The'?   \n",
       "2  I like the movie 'Dumb & Dumber (Dumb and Dumb...   \n",
       "3            What are the ratings for 'Taxi Driver'?   \n",
       "4                  Find all movies released in 1993.   \n",
       "\n",
       "                                    Generated Answer  \\\n",
       "0  The genres of 'Star Wars: Episode IV - A New H...   \n",
       "1                    1 user rated 'Jungle Book, The'   \n",
       "2  Based on the genres of the movie 'Dumb & Dumbe...   \n",
       "3    User 1 rated 'Taxi Driver' with a rating of 5.0   \n",
       "4  The movies released in 1993 are: Three Colors:...   \n",
       "\n",
       "                                     Expected Answer  Cosine Similarity  \\\n",
       "0  The genres of 'Star Wars: Episode IV - A New H...           0.986958   \n",
       "1                  1 users rated 'Jungle Book, The'.           0.975744   \n",
       "2  Based on your preference for 'Dumb & Dumber (D...           0.982890   \n",
       "3             The ratings for 'Taxi Driver' are 5.0.           0.881614   \n",
       "4  The movies released in 1993 are 'Three Colors:...           0.941492   \n",
       "\n",
       "    ROUGE-1   ROUGE-2   ROUGE-L  ROUGE-Lsum      BLEU  Precision_1  \\\n",
       "0  1.000000  0.800000  0.875000    0.875000  0.676192     0.850000   \n",
       "1  0.833333  0.600000  0.833333    0.833333  0.557800     0.857143   \n",
       "2  0.867470  0.814815  0.795181    0.795181  0.762731     0.959184   \n",
       "3  0.421053  0.235294  0.421053    0.421053  0.000000     0.300000   \n",
       "4  0.931507  0.760563  0.876712    0.876712  0.280099     0.615385   \n",
       "\n",
       "   Precision_2  Precision_3  Precision_4  Brevity Penalty  Length Ratio  \\\n",
       "0     0.684211     0.611111     0.588235         1.000000      1.000000   \n",
       "1     0.666667     0.600000     0.500000         0.866878      0.875000   \n",
       "2     0.916667     0.872340     0.847826         0.849366      0.859649   \n",
       "3     0.111111     0.000000     0.000000         1.000000      1.250000   \n",
       "4     0.333333     0.240000     0.183673         0.908324      0.912281   \n",
       "\n",
       "   Translation Length  Reference Length  \n",
       "0                  20                20  \n",
       "1                   7                 8  \n",
       "2                  49                57  \n",
       "3                  10                 8  \n",
       "4                  52                57  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accumulate results\n",
    "results = []\n",
    "\n",
    "for data in evaluation_data:\n",
    "    question = data[\"question\"]\n",
    "    expected_answer = data[\"answer\"]\n",
    "\n",
    "    # Query the system\n",
    "    generated_answer = data[\"LLM\"]  # Using the LLM's response as generated answer\n",
    "    \n",
    "    # Check for specific phrases in the generated answer\n",
    "    if \"User Mistake.\" in generated_answer or \"ERROR: SAFETY Issue.\" in generated_answer:\n",
    "        # Skip score calculation for these responses\n",
    "        continue\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    rouge_result = rouge.compute(predictions=[generated_answer], references=[expected_answer])\n",
    "    bleu_result = bleu.compute(predictions=[generated_answer], references=[[expected_answer]])\n",
    "\n",
    "    # Compute cosine similarity between embeddings\n",
    "    reference_embedding = embed_model.embed_query(expected_answer)\n",
    "    generated_embedding = embed_model.embed_query(generated_answer)\n",
    "    cosine_sim = cosine_similarity([reference_embedding], [generated_embedding])[0][0]\n",
    "\n",
    "    # Append the result as a dictionary\n",
    "    results.append({\n",
    "        \"Question\": question,\n",
    "        \"Generated Answer\": generated_answer,\n",
    "        \"Expected Answer\": expected_answer,\n",
    "        \"Cosine Similarity\": cosine_sim,\n",
    "        # Separate ROUGE metrics\n",
    "        \"ROUGE-1\": rouge_result['rouge1'],\n",
    "        \"ROUGE-2\": rouge_result['rouge2'],\n",
    "        \"ROUGE-L\": rouge_result['rougeL'],\n",
    "        \"ROUGE-Lsum\": rouge_result['rougeLsum'],\n",
    "        # Separate BLEU metrics\n",
    "        \"BLEU\": bleu_result['bleu'],\n",
    "        \"Precision_1\": bleu_result['precisions'][0] if len(bleu_result['precisions']) > 0 else None,\n",
    "        \"Precision_2\": bleu_result['precisions'][1] if len(bleu_result['precisions']) > 1 else None,\n",
    "        \"Precision_3\": bleu_result['precisions'][2] if len(bleu_result['precisions']) > 2 else None,\n",
    "        \"Precision_4\": bleu_result['precisions'][3] if len(bleu_result['precisions']) > 3 else None,\n",
    "        \"Brevity Penalty\": bleu_result['brevity_penalty'],\n",
    "        \"Length Ratio\": bleu_result['length_ratio'],\n",
    "        \"Translation Length\": bleu_result['translation_length'],\n",
    "        \"Reference Length\": bleu_result['reference_length'],\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for tabular results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: 0.7230897798298865\n",
      "Average ROUGE-2: 0.5811408347565529\n",
      "Average ROUGE-L: 0.6995416412239113\n",
      "Average ROUGE-Lsum: 0.6995416412239113\n",
      "Average BLEU: 0.3841377879656104\n",
      "Average Cosine Similarity: 0.830061561491707\n"
     ]
    }
   ],
   "source": [
    "# Print the average scores\n",
    "print(f\"Average ROUGE-1: {result_df['ROUGE-1'].mean()}\")\n",
    "print(f\"Average ROUGE-2: {result_df['ROUGE-2'].mean()}\")\n",
    "print(f\"Average ROUGE-L: {result_df['ROUGE-L'].mean()}\")\n",
    "print(f\"Average ROUGE-Lsum: {result_df['ROUGE-Lsum'].mean()}\")\n",
    "print(f\"Average BLEU: {result_df['BLEU'].mean()}\")\n",
    "print(f\"Average Cosine Similarity: {result_df['Cosine Similarity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevance_scores, k):\n",
    "    \"\"\"\n",
    "    Compute the Discounted Cumulative Gain (DCG) at rank k.\n",
    "    relevance_scores: list of relevance scores (binary or graded)\n",
    "    k: rank cutoff\n",
    "    \"\"\"\n",
    "    relevance_scores = np.asarray(relevance_scores, dtype=float)[:k]\n",
    "    if relevance_scores.size:\n",
    "        return np.sum((2**relevance_scores - 1) / np.log2(np.arange(1, relevance_scores.size + 1) + 1))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(ground_truth, response, k):\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain (NDCG) at rank k.\n",
    "    ground_truth: list of ground truth (expected) movies\n",
    "    response: list of movies returned by the LLM\n",
    "    k: rank cutoff\n",
    "    \"\"\"\n",
    "    # Binary relevance scores (1 if the movie is in ground truth, 0 otherwise)\n",
    "    relevance_scores = [1 if movie in ground_truth else 0 for movie in response]\n",
    "    \n",
    "    # Compute DCG for the LLM's response\n",
    "    dcg = dcg_at_k(relevance_scores, k)\n",
    "    \n",
    "    # Ideal relevance scores (all 1s for the length of ground_truth, since all movies are relevant in ground truth)\n",
    "    ideal_relevance_scores = [1] * min(k, len(ground_truth))\n",
    "    \n",
    "    # Compute Ideal DCG (IDCG)\n",
    "    idcg = dcg_at_k(ideal_relevance_scores, k)\n",
    "    \n",
    "    # Return NDCG (if IDCG is 0, NDCG is 0 to avoid division by zero)\n",
    "    return dcg / idcg if idcg > 0 else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  1 NDCG: 0.8235294117647058\n",
      "For k =  3 NDCG: 0.8235294117647058\n",
      "For k =  5 NDCG: 0.8244037219017861\n",
      "For k =  10 NDCG: 0.8333657571496348\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10]:\n",
    "    NDCG_results = []\n",
    "    for data in evaluation_data:\n",
    "        ground_truth = data[\"GT_NDCG\"]\n",
    "        response = data[\"response_NDCG\"]\n",
    "        # Query the system\n",
    "        generated_answer = data[\"LLM\"]  # Using the LLM's response as generated answer\n",
    "\n",
    "        # Check for specific phrases in the generated answer\n",
    "        if \"User Mistake.\" in generated_answer or \"ERROR: SAFETY Issue.\" in generated_answer:\n",
    "            # Skip score calculation for these responses\n",
    "            continue\n",
    "\n",
    "        # Calculate NDCG score at rank 3\n",
    "        # k = 3\n",
    "        ndcg_score = ndcg_at_k(ground_truth, response, k)\n",
    "        NDCG_results.append(ndcg_score)\n",
    "    print(\"For k = \", k, \"NDCG:\", np.mean(NDCG_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_at_k(gt_list, response_list, k):\n",
    "    # Limit both lists to the top K\n",
    "    gt_k = set(gt_list[:k])\n",
    "    response_k = set(response_list[:k])\n",
    "    \n",
    "    # Calculate Precision@K\n",
    "    relevant_and_retrieved = gt_k.intersection(response_k)\n",
    "    precision_k = len(relevant_and_retrieved) / len(response_k) if response_k else 0\n",
    "    \n",
    "    # Calculate Recall@K\n",
    "    recall_k = len(relevant_and_retrieved) / len(gt_k) if gt_k else 0\n",
    "    \n",
    "    # Calculate F1@K\n",
    "    if precision_k + recall_k == 0:\n",
    "        f1_k = 0\n",
    "    else:\n",
    "        f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k)\n",
    "    \n",
    "    # Calculate Rate@K\n",
    "    rate_k = len(relevant_and_retrieved) / k\n",
    "    \n",
    "    return precision_k, recall_k, f1_k, rate_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_at_k_with_adjustment(gt_list, response_list, k):\n",
    "    # Adjust k if it's larger than the available elements in gt_list or response_list\n",
    "    k = min(k, len(gt_list), len(response_list))\n",
    "    \n",
    "    # Limit both lists to the top K\n",
    "    gt_k = set(gt_list[:k])\n",
    "    response_k = set(response_list[:k])\n",
    "    \n",
    "    # Calculate Precision@K\n",
    "    relevant_and_retrieved = gt_k.intersection(response_k)\n",
    "    precision_k = len(relevant_and_retrieved) / len(response_k) if response_k else 0\n",
    "    \n",
    "    # Calculate Recall@K\n",
    "    recall_k = len(relevant_and_retrieved) / len(gt_k) if gt_k else 0\n",
    "    \n",
    "    # Calculate F1@K\n",
    "    if precision_k + recall_k == 0:\n",
    "        f1_k = 0\n",
    "    else:\n",
    "        f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k)\n",
    "    \n",
    "    # Calculate Rate@K\n",
    "    rate_k = len(relevant_and_retrieved) / k if k > 0 else 0\n",
    "    \n",
    "    return precision_k, recall_k, f1_k, rate_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of responses: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of responses:\", len(evaluation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  1 Precision: 0.7368421052631579\n",
      "For k =  1 Recall: 0.7368421052631579\n",
      "For k =  1 F1: 0.7368421052631579\n",
      "For k =  1 Rate: 0.7368421052631579\n",
      "--------------------------------------------------\n",
      "For k =  3 Precision: 0.7368421052631579\n",
      "For k =  3 Recall: 0.7368421052631579\n",
      "For k =  3 F1: 0.7368421052631579\n",
      "For k =  3 Rate: 0.5263157894736842\n",
      "--------------------------------------------------\n",
      "For k =  5 Precision: 0.7368421052631579\n",
      "For k =  5 Recall: 0.7368421052631579\n",
      "For k =  5 F1: 0.7368421052631579\n",
      "For k =  5 Rate: 0.46315789473684216\n",
      "--------------------------------------------------\n",
      "For k =  10 Precision: 0.7508771929824561\n",
      "For k =  10 Recall: 0.7473684210526316\n",
      "For k =  10 F1: 0.7490304709141274\n",
      "For k =  10 Rate: 0.30526315789473685\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "k = 5  # You can adjust K as needed\n",
    "for k in [1, 3, 5, 10]:\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    rate_scores = []\n",
    "    for data in evaluation_data:\n",
    "        ground_truth = data[\"GT_NDCG\"]\n",
    "        response = data[\"response_NDCG\"]\n",
    "        # Query the system\n",
    "        generated_answer = data[\"LLM\"] \n",
    "\n",
    "        # Calculate metrics at rank K\n",
    "        precision_k, recall_k, f1_k, rate_k = calculate_metrics_at_k(ground_truth, response, k)\n",
    "        precision_scores.append(precision_k)\n",
    "        recall_scores.append(recall_k)\n",
    "        f1_scores.append(f1_k)\n",
    "        rate_scores.append(rate_k)\n",
    "    print(\"For k = \", k, \"Precision:\", np.mean(precision_scores))\n",
    "    print(\"For k = \", k, \"Recall:\", np.mean(recall_scores))\n",
    "    print(\"For k = \", k, \"F1:\", np.mean(f1_scores))\n",
    "    print(\"For k = \", k, \"Rate:\", np.mean(rate_scores))\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
