{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pprint\n",
    "import json\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open('results/QA_RESEARCH_Gemini.json', 'r') as f:\n",
    "    evaluation_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hc/dq1y9hzx51s30kq78z6v4jsm0000gp/T/ipykernel_34899/3666243940.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Accumulate results\n",
    "rouge_scores = []\n",
    "bleu_scores = []\n",
    "cosine_similarities = []\n",
    "\n",
    "for data in evaluation_data:\n",
    "    question = data[\"question\"]\n",
    "    expected_answer = data[\"answer\"]\n",
    "    generated_answer = data[\"LLM\"]  # Use the generated answer from the JSON data\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    rouge_result = rouge.compute(predictions=[generated_answer], references=[expected_answer])\n",
    "    bleu_result = bleu.compute(predictions=[generated_answer], references=[[expected_answer]])\n",
    "\n",
    "    # Compute cosine similarity between embeddings\n",
    "    reference_embedding = embed_model.embed_query(expected_answer)\n",
    "    generated_embedding = embed_model.embed_query(generated_answer)\n",
    "    cosine_sim = cosine_similarity([reference_embedding], [generated_embedding])[0][0]\n",
    "\n",
    "    # Store the results\n",
    "    rouge_scores.append(rouge_result)\n",
    "    bleu_scores.append(bleu_result)\n",
    "    cosine_similarities.append(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Generated Answer</th>\n",
       "      <th>Expected Answer</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>ROUGE-Lsum</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_2</th>\n",
       "      <th>Precision_3</th>\n",
       "      <th>Precision_4</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List the titles of papers authored by 'Jianmin...</td>\n",
       "      <td>Jianmin Chen authored the paper 'TensorFlow: A...</td>\n",
       "      <td>Jianmin Chen authored the paper 'TensorFlow: A...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the most cited papers in 'Physics'?</td>\n",
       "      <td>The most cited papers related to Physics are: ...</td>\n",
       "      <td>The most cited papers related to Physics are: ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which authors have worked on the 'Network Scie...</td>\n",
       "      <td>The following authors have worked on the 'Netw...</td>\n",
       "      <td>The following authors have worked on the 'Netw...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What venues have published papers in 'Environm...</td>\n",
       "      <td>PLoS ONE.</td>\n",
       "      <td>PLoS ONE has published papers in 'Environmenta...</td>\n",
       "      <td>0.189857</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096972</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many papers authored by 'Roland Vollgraf'?</td>\n",
       "      <td>Roland Vollgraf authored 1 paper.</td>\n",
       "      <td>Roland Vollgraf authored 1 paper.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  List the titles of papers authored by 'Jianmin...   \n",
       "1       What are the most cited papers in 'Physics'?   \n",
       "2  Which authors have worked on the 'Network Scie...   \n",
       "3  What venues have published papers in 'Environm...   \n",
       "4     How many papers authored by 'Roland Vollgraf'?   \n",
       "\n",
       "                                    Generated Answer  \\\n",
       "0  Jianmin Chen authored the paper 'TensorFlow: A...   \n",
       "1  The most cited papers related to Physics are: ...   \n",
       "2  The following authors have worked on the 'Netw...   \n",
       "3                                          PLoS ONE.   \n",
       "4                  Roland Vollgraf authored 1 paper.   \n",
       "\n",
       "                                     Expected Answer  Cosine Similarity  \\\n",
       "0  Jianmin Chen authored the paper 'TensorFlow: A...           1.000000   \n",
       "1  The most cited papers related to Physics are: ...           1.000000   \n",
       "2  The following authors have worked on the 'Netw...           1.000000   \n",
       "3  PLoS ONE has published papers in 'Environmenta...           0.189857   \n",
       "4                  Roland Vollgraf authored 1 paper.           1.000000   \n",
       "\n",
       "    ROUGE-1   ROUGE-2   ROUGE-L  ROUGE-Lsum  BLEU  Precision_1  Precision_2  \\\n",
       "0  1.000000  1.000000  1.000000    1.000000   1.0          1.0          1.0   \n",
       "1  1.000000  1.000000  1.000000    1.000000   1.0          1.0          1.0   \n",
       "2  1.000000  1.000000  1.000000    1.000000   1.0          1.0          1.0   \n",
       "3  0.363636  0.222222  0.363636    0.363636   0.0          1.0          0.5   \n",
       "4  1.000000  1.000000  1.000000    1.000000   1.0          1.0          1.0   \n",
       "\n",
       "   Precision_3  Precision_4  Brevity Penalty  Length Ratio  \\\n",
       "0          1.0          1.0         1.000000           1.0   \n",
       "1          1.0          1.0         1.000000           1.0   \n",
       "2          1.0          1.0         1.000000           1.0   \n",
       "3          0.0          0.0         0.096972           0.3   \n",
       "4          1.0          1.0         1.000000           1.0   \n",
       "\n",
       "   Translation Length  Reference Length  \n",
       "0                  16                16  \n",
       "1                 120               120  \n",
       "2                  51                51  \n",
       "3                   3                10  \n",
       "4                   6                 6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accumulate results\n",
    "results = []\n",
    "\n",
    "for data in evaluation_data:\n",
    "    question = data[\"question\"]\n",
    "    expected_answer = data[\"answer\"]\n",
    "\n",
    "    # Query the system\n",
    "    generated_answer = data[\"LLM\"]  # Using the LLM's response as generated answer\n",
    "    \n",
    "    # Check for specific phrases in the generated answer\n",
    "    if \"User Mistake.\" in generated_answer or \"ERROR: SAFETY Issue.\" in generated_answer:\n",
    "        # Skip score calculation for these responses\n",
    "        continue\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    rouge_result = rouge.compute(predictions=[generated_answer], references=[expected_answer])\n",
    "    bleu_result = bleu.compute(predictions=[generated_answer], references=[[expected_answer]])\n",
    "\n",
    "    # Compute cosine similarity between embeddings\n",
    "    reference_embedding = embed_model.embed_query(expected_answer)\n",
    "    generated_embedding = embed_model.embed_query(generated_answer)\n",
    "    cosine_sim = cosine_similarity([reference_embedding], [generated_embedding])[0][0]\n",
    "\n",
    "    # Append the result as a dictionary\n",
    "    results.append({\n",
    "        \"Question\": question,\n",
    "        \"Generated Answer\": generated_answer,\n",
    "        \"Expected Answer\": expected_answer,\n",
    "        \"Cosine Similarity\": cosine_sim,\n",
    "        # Separate ROUGE metrics\n",
    "        \"ROUGE-1\": rouge_result['rouge1'],\n",
    "        \"ROUGE-2\": rouge_result['rouge2'],\n",
    "        \"ROUGE-L\": rouge_result['rougeL'],\n",
    "        \"ROUGE-Lsum\": rouge_result['rougeLsum'],\n",
    "        # Separate BLEU metrics\n",
    "        \"BLEU\": bleu_result['bleu'],\n",
    "        \"Precision_1\": bleu_result['precisions'][0] if len(bleu_result['precisions']) > 0 else None,\n",
    "        \"Precision_2\": bleu_result['precisions'][1] if len(bleu_result['precisions']) > 1 else None,\n",
    "        \"Precision_3\": bleu_result['precisions'][2] if len(bleu_result['precisions']) > 2 else None,\n",
    "        \"Precision_4\": bleu_result['precisions'][3] if len(bleu_result['precisions']) > 3 else None,\n",
    "        \"Brevity Penalty\": bleu_result['brevity_penalty'],\n",
    "        \"Length Ratio\": bleu_result['length_ratio'],\n",
    "        \"Translation Length\": bleu_result['translation_length'],\n",
    "        \"Reference Length\": bleu_result['reference_length'],\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for tabular results\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: 0.8008468932622385\n",
      "Average ROUGE-2: 0.7698130380205725\n",
      "Average ROUGE-L: 0.7697030804819576\n",
      "Average ROUGE-Lsum: 0.7697030804819576\n",
      "Average BLEU: 0.7216656980705826\n",
      "Average Cosine Similarity: 0.8337880092016062\n"
     ]
    }
   ],
   "source": [
    "# Print the average scores\n",
    "print(f\"Average ROUGE-1: {result_df['ROUGE-1'].mean()}\")\n",
    "print(f\"Average ROUGE-2: {result_df['ROUGE-2'].mean()}\")\n",
    "print(f\"Average ROUGE-L: {result_df['ROUGE-L'].mean()}\")\n",
    "print(f\"Average ROUGE-Lsum: {result_df['ROUGE-Lsum'].mean()}\")\n",
    "print(f\"Average BLEU: {result_df['BLEU'].mean()}\")\n",
    "print(f\"Average Cosine Similarity: {result_df['Cosine Similarity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevance_scores, k):\n",
    "    \"\"\"\n",
    "    Compute the Discounted Cumulative Gain (DCG) at rank k.\n",
    "    relevance_scores: list of relevance scores (binary or graded)\n",
    "    k: rank cutoff\n",
    "    \"\"\"\n",
    "    relevance_scores = np.asarray(relevance_scores, dtype=float)[:k]\n",
    "    if relevance_scores.size:\n",
    "        return np.sum((2**relevance_scores - 1) / np.log2(np.arange(1, relevance_scores.size + 1) + 1))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(ground_truth, response, k):\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain (NDCG) at rank k.\n",
    "    ground_truth: list of ground truth (expected) movies\n",
    "    response: list of movies returned by the LLM\n",
    "    k: rank cutoff\n",
    "    \"\"\"\n",
    "    # Binary relevance scores (1 if the movie is in ground truth, 0 otherwise)\n",
    "    relevance_scores = [1 if movie in ground_truth else 0 for movie in response]\n",
    "    \n",
    "    # Compute DCG for the LLM's response\n",
    "    dcg = dcg_at_k(relevance_scores, k)\n",
    "    \n",
    "    # Ideal relevance scores (all 1s for the length of ground_truth, since all movies are relevant in ground truth)\n",
    "    ideal_relevance_scores = [1] * min(k, len(ground_truth))\n",
    "    \n",
    "    # Compute Ideal DCG (IDCG)\n",
    "    idcg = dcg_at_k(ideal_relevance_scores, k)\n",
    "    \n",
    "    # Return NDCG (if IDCG is 0, NDCG is 0 to avoid division by zero)\n",
    "    return dcg / idcg if idcg > 0 else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  1 NDCG: 0.8421052631578947\n",
      "For k =  3 NDCG: 0.8421052631578947\n",
      "For k =  5 NDCG: 0.8421052631578947\n",
      "For k =  10 NDCG: 0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10]:\n",
    "    NDCG_results = []\n",
    "    for data in evaluation_data:\n",
    "        ground_truth = data[\"GT_NDCG\"]\n",
    "        response = data[\"response_NDCG\"]\n",
    "        # Query the system\n",
    "        generated_answer = data[\"LLM\"]  # Using the LLM's response as generated answer\n",
    "\n",
    "        # Check for specific phrases in the generated answer\n",
    "        if \"User Mistake.\" in generated_answer or \"ERROR: SAFETY Issue.\" in generated_answer:\n",
    "            # Skip score calculation for these responses\n",
    "            continue\n",
    "\n",
    "        # Calculate NDCG score at rank 3\n",
    "        # k = 3\n",
    "        ndcg_score = ndcg_at_k(ground_truth, response, k)\n",
    "        NDCG_results.append(ndcg_score)\n",
    "    print(\"For k = \", k, \"NDCG:\", np.mean(NDCG_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_at_k(gt_list, response_list, k):\n",
    "    # Limit both lists to the top K\n",
    "    gt_k = set(gt_list[:k])\n",
    "    response_k = set(response_list[:k])\n",
    "    \n",
    "    # Calculate Precision@K\n",
    "    relevant_and_retrieved = gt_k.intersection(response_k)\n",
    "    precision_k = len(relevant_and_retrieved) / len(response_k) if response_k else 0\n",
    "    \n",
    "    # Calculate Recall@K\n",
    "    recall_k = len(relevant_and_retrieved) / len(gt_k) if gt_k else 0\n",
    "    \n",
    "    # Calculate F1@K\n",
    "    if precision_k + recall_k == 0:\n",
    "        f1_k = 0\n",
    "    else:\n",
    "        f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k)\n",
    "    \n",
    "    # Calculate Rate@K\n",
    "    rate_k = len(relevant_and_retrieved) / k\n",
    "    \n",
    "    return precision_k, recall_k, f1_k, rate_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_at_k_with_adjustment(gt_list, response_list, k):\n",
    "    # Adjust k if it's larger than the available elements in gt_list or response_list\n",
    "    k = min(k, len(gt_list), len(response_list))\n",
    "    \n",
    "    # Limit both lists to the top K\n",
    "    gt_k = set(gt_list[:k])\n",
    "    response_k = set(response_list[:k])\n",
    "    \n",
    "    # Calculate Precision@K\n",
    "    relevant_and_retrieved = gt_k.intersection(response_k)\n",
    "    precision_k = len(relevant_and_retrieved) / len(response_k) if response_k else 0\n",
    "    \n",
    "    # Calculate Recall@K\n",
    "    recall_k = len(relevant_and_retrieved) / len(gt_k) if gt_k else 0\n",
    "    \n",
    "    # Calculate F1@K\n",
    "    if precision_k + recall_k == 0:\n",
    "        f1_k = 0\n",
    "    else:\n",
    "        f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k)\n",
    "    \n",
    "    # Calculate Rate@K\n",
    "    rate_k = len(relevant_and_retrieved) / k if k > 0 else 0\n",
    "    \n",
    "    return precision_k, recall_k, f1_k, rate_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of responses: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of responses:\", len(evaluation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  1 Precision: 0.8421052631578947\n",
      "For k =  1 Recall: 0.8421052631578947\n",
      "For k =  1 F1: 0.8421052631578947\n",
      "For k =  1 Rate: 0.8421052631578947\n",
      "--------------------------------------------------\n",
      "For k =  3 Precision: 0.8421052631578947\n",
      "For k =  3 Recall: 0.8421052631578947\n",
      "For k =  3 F1: 0.8421052631578947\n",
      "For k =  3 Rate: 0.5789473684210527\n",
      "--------------------------------------------------\n",
      "For k =  5 Precision: 0.8421052631578947\n",
      "For k =  5 Recall: 0.8421052631578947\n",
      "For k =  5 F1: 0.8421052631578947\n",
      "For k =  5 Rate: 0.5157894736842105\n",
      "--------------------------------------------------\n",
      "For k =  10 Precision: 0.8421052631578947\n",
      "For k =  10 Recall: 0.8421052631578947\n",
      "For k =  10 F1: 0.8421052631578947\n",
      "For k =  10 Rate: 0.4052631578947368\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "k = 5  # You can adjust K as needed\n",
    "for k in [1, 3, 5, 10]:\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    rate_scores = []\n",
    "    for data in evaluation_data:\n",
    "        ground_truth = data[\"GT_NDCG\"]\n",
    "        response = data[\"response_NDCG\"]\n",
    "        # Query the system\n",
    "        generated_answer = data[\"LLM\"] \n",
    "\n",
    "        # Calculate metrics at rank K\n",
    "        precision_k, recall_k, f1_k, rate_k = calculate_metrics_at_k(ground_truth, response, k)\n",
    "        precision_scores.append(precision_k)\n",
    "        recall_scores.append(recall_k)\n",
    "        f1_scores.append(f1_k)\n",
    "        rate_scores.append(rate_k)\n",
    "    print(\"For k = \", k, \"Precision:\", np.mean(precision_scores))\n",
    "    print(\"For k = \", k, \"Recall:\", np.mean(recall_scores))\n",
    "    print(\"For k = \", k, \"F1:\", np.mean(f1_scores))\n",
    "    print(\"For k = \", k, \"Rate:\", np.mean(rate_scores))\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
