{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b977548",
   "metadata": {},
   "source": [
    "## Research Influencer Identification Notebook\n",
    "#\n",
    "**Objective:** Identify influential researchers within a dataset of research awards based on collaboration patterns and research breadth.\n",
    "#\n",
    "**Methodology:**\n",
    "1.  **Load Data:** Read award data from JSON files, extracting relevant information about awards, organizations, programs, and investigators (PIs).\n",
    "2.  **Clean & Preprocess:** Create a Pandas DataFrame, clean the data (handle missing values, filter roles), and perform feature engineering (leadership indicator, experience).\n",
    "3.  **Generate Embeddings:** Create text embeddings for each award's descriptive text using a Sentence Transformer model (`all-MiniLM-L6-v2`) to capture semantic meaning.\n",
    "4.  **Aggregate PI Data:** Group the data by Principal Investigator (PI) ID, aggregating metrics like average experience, leadership status, total awards, and average text embedding.\n",
    "5.  **Normalize Features:** Scale numeric features (experience, award count) for fair comparison.\n",
    "6.  **Candidate Selection:** Implement functions to select a pool of candidate PIs based on specific criteria:\n",
    "    * **Topic Similarity:** Find PIs whose aggregated research text embeddings are most similar to a given research topic.\n",
    "    * **Department:** Find PIs associated with a specific academic department, potentially ranking them by award count.\n",
    "7.  **LLM-Based Ranking:**\n",
    "    * Format the aggregated data (project count, unique collaborators, field diversity) for the selected candidates into a structured text format.\n",
    "    * Generate a prompt instructing a Large Language Model (LLM - Google's Gemini) to rank these candidates based on the provided metrics and a definition of \"influence\" (project volume, collaborator network size, field diversity).\n",
    "    * Send the prompt to the LLM and retrieve the ranked list with justifications.\n",
    "8.  **Verification:** Implement a function to independently recalculate the key metrics (project count, collaborator count, field count) for a specific PI directly from the source data to verify the numbers used in the LLM ranking.\n",
    "#\n",
    "**Libraries Used:**\n",
    "* `dotenv`: To load environment variables (like API keys).\n",
    "* `os`: For file system operations (listing directories/files).\n",
    "* `google.generativeai`: Google Gemini API client.\n",
    "* `IPython.display`: For displaying rich content in notebooks (optional).\n",
    "* `time`: For timing API calls.\n",
    "* `typing`: For type hinting.\n",
    "* `datetime`: For calculating experience based on dates.\n",
    "* `sentence_transformers`: For generating text embeddings.\n",
    "* `sklearn.metrics.pairwise`: For calculating cosine similarity (for topic search).\n",
    "* `sklearn.preprocessing`: For data normalization (MinMaxScaler).\n",
    "* `json`: For reading JSON data files.\n",
    "* `pandas`: For data manipulation and analysis.\n",
    "* `numpy`: For numerical operations, especially with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8afdd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional # For type hinting\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler # Import MinMaxScaler here\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cac40139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "## Load Gemini model\n",
    "# Using a known stable model, replace if needed\n",
    "try:\n",
    "    # model=genai.GenerativeModel('gemini-1.0-pro')\n",
    "    # Updated model name based on your original code\n",
    "    model=genai.GenerativeModel('gemini-2.0-flash-thinking-exp-01-21')\n",
    "    print(\"Gemini model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Gemini model: {e}\")\n",
    "    exit() # Exit if model cannot be loaded\n",
    "\n",
    "data_directory = 'data/ranking_data/'\n",
    "records = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c348b4",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Parsing\n",
    "\n",
    "This section reads JSON files containing award data. Each file typically represents one award and may contain information about multiple investigators (PIs) associated with it.\n",
    "\n",
    "We define a helper function `safe_get` to navigate potentially missing keys or incorrect structures within the JSON data gracefully. The code iterates through subdirectories and files, extracts key fields for each award and associated PI, and stores them in a list of dictionaries (`records`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1596628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get(data, keys, default=None):\n",
    "    \"\"\"\n",
    "    Safely get a nested key from a dictionary using a list of keys.\n",
    "    Returns default if any key is missing.\n",
    "    \"\"\"\n",
    "    temp_data = data\n",
    "    for key in keys:\n",
    "        if isinstance(temp_data, dict) and key in temp_data:\n",
    "            temp_data = temp_data[key]\n",
    "        # Allow lists if they are not the final step\n",
    "        elif isinstance(temp_data, list) and key < len(temp_data) and key >= 0:\n",
    "             temp_data = temp_data[key]\n",
    "        else:\n",
    "            # print(f\"Debug: Key '{key}' not found or invalid index in path {keys} within data: {data}\")\n",
    "            return default\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f79f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading from: data/ranking_data/\n",
      "Reading files in 2022...\n",
      "Reading files in 2024...\n",
      "Reading files in 2023...\n",
      "Reading files in 2021...\n",
      "Reading files in 2020...\n",
      "Finished reading files. Total records loaded: 89056\n",
      "DataFrame created with shape: (89056, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting data loading from: {data_directory}\")\n",
    "if not os.path.isdir(data_directory):\n",
    "    print(f\"Error: Data directory '{data_directory}' not found.\")\n",
    "    exit()\n",
    "    \n",
    "for sub_dir in os.listdir(data_directory):\n",
    "    sub_directory_path = os.path.join(data_directory, sub_dir)\n",
    "    if not os.path.isdir(sub_directory_path):\n",
    "        continue # Skip if it's not a directory\n",
    "\n",
    "    print(f\"Reading files in {sub_dir}...\")\n",
    "    for filename in os.listdir(sub_directory_path):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(sub_directory_path, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as file: # Specify encoding\n",
    "                    data = json.load(file)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filepath}: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Extract award-level context information safely\n",
    "            award_type = data.get(\"awd_istr_txt\")\n",
    "            award_title = data.get(\"awd_titl_txt\")\n",
    "            abstract = data.get(\"abst_narr_txt\")\n",
    "            org_name = safe_get(data, [\"org\", \"org_long_name\"]) # Adjusted path based on potential structure\n",
    "            # org_name2 = data.get(\"org_long_name2\") # Keep if exists, otherwise remove or use safe_get\n",
    "            perf_inst_name = safe_get(data, [\"perf_inst\", \"perf_inst_name\"])\n",
    "\n",
    "            # Extract program element and reference safely (checking if list exists)\n",
    "            pgm_ele_list = data.get(\"pgm_ele\")\n",
    "            program_element = None\n",
    "            if isinstance(pgm_ele_list, list) and len(pgm_ele_list) > 0:\n",
    "                 program_element = safe_get(pgm_ele_list, [0, \"pgm_ele_long_name\"]) # Safe get within list\n",
    "\n",
    "            pgm_ref_list = data.get(\"pgm_ref\")\n",
    "            program_reference = None\n",
    "            if isinstance(pgm_ref_list, list) and len(pgm_ref_list) > 0:\n",
    "                 program_reference = safe_get(pgm_ref_list, [0, \"pgm_ref_long_name\"]) # Safe get within list\n",
    "\n",
    "            # Get investigator information, ensuring it's a list\n",
    "            pi_list = data.get(\"pi\")\n",
    "            if not isinstance(pi_list, list):\n",
    "                # print(f\"Skipping file {filename}: 'pi' data is not a list.\")\n",
    "                continue # Skip if 'pi' is not a list\n",
    "\n",
    "            # Loop through each investigator in the file\n",
    "            for pi in pi_list:\n",
    "                 # Ensure pi is a dictionary before trying to get keys\n",
    "                 if not isinstance(pi, dict):\n",
    "                     # print(f\"Skipping PI entry in {filename}: Entry is not a dictionary: {pi}\")\n",
    "                     continue\n",
    "\n",
    "                 record = {\n",
    "                    \"award_type\": award_type,\n",
    "                    \"award_title\": award_title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"org_name\": org_name,\n",
    "                    # \"org_name2\": org_name2, # Remove if not consistently present\n",
    "                    \"perf_inst_name\": perf_inst_name,\n",
    "                    \"program_element\": program_element,\n",
    "                    \"program_reference\": program_reference,\n",
    "                    \"pi_id\": pi.get(\"pi_id\"),\n",
    "                    \"pi_full_name\": pi.get(\"pi_full_name\", \"\").strip() if pi.get(\"pi_full_name\") else None,\n",
    "                    \"role\": pi.get(\"proj_role_code2\", \"\").strip() if pi.get(\"proj_role_code2\") else None,\n",
    "                    \"department\": pi.get(\"pi_dept_name\"),\n",
    "                    \"email\": pi.get(\"pi_email_addr\"),\n",
    "                    \"start_date\": pi.get(\"start_date\")\n",
    "                }\n",
    "                 records.append(record)\n",
    "\n",
    "print(f\"Finished reading files. Total records loaded: {len(records)}\")\n",
    "\n",
    "# Create a DataFrame from the records\n",
    "if not records:\n",
    "    print(\"Error: No records were loaded. Cannot create DataFrame.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"DataFrame created with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2038a0",
   "metadata": {},
   "source": [
    "## DataFrame Creation and Preprocessing\n",
    "\n",
    "Converts the list of records into a Pandas DataFrame. Performs essential cleaning:\n",
    "* Drops records missing a `pi_id`.\n",
    "* Filters the DataFrame to include only records where the investigator's role is 'Principal Investigator' or 'Co-Principal Investigator', as these roles are central to the influence analysis.\n",
    "* Handles potential `NaN` values in text columns before combining them.\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Creates new features based on the existing data:\n",
    "* **`combined_text`**: Concatenates relevant textual fields (title, abstract, program names, etc.) into a single string for each record. This text will be used to generate embeddings. Missing values in text columns are filled with empty strings before concatenation.\n",
    "* **`leadership`**: A binary indicator (1 or 0) set to 1 if the PI's role is 'Principal Investigator', signifying primary leadership on the award.\n",
    "* **`experience_years`**: Calculates the approximate years of experience based on the award's `start_date` relative to the current date. Handles potential errors in date conversion and fills missing experience values with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6384bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping rows with missing pi_id: (89056, 15)\n",
      "Shape after filtering for PI/Co-PI roles: (83112, 15)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Cleaning and Preprocessing ---\n",
    "# Drop rows where pi_id is missing, as it's crucial for grouping\n",
    "df.dropna(subset=['pi_id'], inplace=True)\n",
    "print(f\"Shape after dropping rows with missing pi_id: {df.shape}\")\n",
    "\n",
    "# Filter for relevant roles\n",
    "df = df[df['role'].isin(['Co-Principal Investigator', 'Principal Investigator'])].copy() # Use .copy()\n",
    "print(f\"Shape after filtering for PI/Co-PI roles: {df.shape}\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"Error: DataFrame is empty after filtering for PI/Co-PI roles. Check data or filters.\")\n",
    "    exit()\n",
    "\n",
    "# Combine relevant text columns into one\n",
    "text_columns = [\n",
    "    \"award_type\", \"award_title\", \"abstract\",\n",
    "    \"org_name\", #\"org_name2\", # Remove if not used\n",
    "    \"perf_inst_name\",\n",
    "    \"program_element\", \"program_reference\"\n",
    "]\n",
    "# Fill NaN values with empty strings before combining\n",
    "for col in text_columns:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Warning: Column '{col}' not found in DataFrame. Skipping for combined_text.\")\n",
    "        text_columns.remove(col) # Remove if it doesn't exist\n",
    "    else:\n",
    "        df[col] = df[col].fillna('') # Fill NaNs specifically\n",
    "\n",
    "df[\"combined_text\"] = df[text_columns].astype(str).agg(\" \".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0a2e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hc/dq1y9hzx51s30kq78z6v4jsm0000gp/T/ipykernel_5510/3718463765.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"experience_years\"].fillna(0, inplace=True) # Handle potential NaNs from bad dates\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# a. Leadership indicator\n",
    "df[\"leadership\"] = df[\"role\"].apply(lambda x: 1 if \"Principal Investigator\" in str(x) else 0)\n",
    "\n",
    "# b. Experience in years\n",
    "df[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors='coerce')\n",
    "reference_date = datetime.now()\n",
    "df[\"experience_years\"] = (reference_date - df[\"start_date\"]).dt.days / 365.25\n",
    "df[\"experience_years\"].fillna(0, inplace=True) # Handle potential NaNs from bad dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb30ce",
   "metadata": {},
   "source": [
    "## Text Embedding Generation\n",
    "Uses a pre-trained Sentence Transformer model (`all-MiniLM-L6-v2`) to convert the `combined_text` for each award record into a dense vector representation (embedding). These embeddings capture the semantic meaning of the text. Error handling is included for cases where text cannot be embedded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db62499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load Sentence Transformer\n",
    "try:\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    print(\"SentenceTransformer model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SentenceTransformer model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Compute embeddings (handle potential errors during embedding)\n",
    "embeddings = []\n",
    "for text in df[\"combined_text\"]:\n",
    "    try:\n",
    "        embeddings.append(embedder.encode(text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding text: {text[:100]}... Error: {e}\")\n",
    "        # Append a zero vector or handle as appropriate\n",
    "        embeddings.append(np.zeros(embedder.get_sentence_embedding_dimension()))\n",
    "df[\"text_embedding\"] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a643dc9",
   "metadata": {},
   "source": [
    "## Data Aggregation by PI\n",
    "\n",
    "Groups the DataFrame by `pi_id` to create a summary profile for each unique investigator. It aggregates the features calculated earlier:\n",
    "* `award_count`: Total number of awards the PI is listed on (as PI or Co-PI).\n",
    "* `experience_years`: Average experience across all their awards.\n",
    "* `leadership`: Maximum value of the leadership indicator (1 if they were a PI on at least one award, 0 otherwise).\n",
    "* `text_embedding`: Average of the text embeddings from all their awards. This creates a single vector representing the PI's overall research area(s).\n",
    "\n",
    "Error handling is included for cases where a PI might have missing embedding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db8e57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping data by PI ID...\n",
      "Grouped DataFrame created with shape: (54006, 5)\n"
     ]
    }
   ],
   "source": [
    "# Group by PI\n",
    "print(\"Grouping data by PI ID...\")\n",
    "award_counts = df.groupby(\"pi_id\").size().reset_index(name=\"award_count\")\n",
    "\n",
    "# Ensure text_embedding exists before aggregation\n",
    "if \"text_embedding\" not in df.columns:\n",
    "     print(\"Error: 'text_embedding' column not found before grouping.\")\n",
    "     # Handle error appropriately, maybe exit or create dummy embeddings\n",
    "     exit()\n",
    "\n",
    "# Check for empty groups or non-numeric data before aggregation\n",
    "numeric_cols_agg = {\n",
    "    \"experience_years\": \"mean\",\n",
    "    \"leadership\": \"max\"\n",
    "}\n",
    "lambda_agg = {\n",
    "    \"text_embedding\": lambda embs: np.mean(np.stack(embs), axis=0) if len(embs) > 0 else np.zeros(embedder.get_sentence_embedding_dimension())\n",
    "}\n",
    "\n",
    "# Perform aggregation\n",
    "df_grouped = df.groupby(\"pi_id\").agg({**numeric_cols_agg, **lambda_agg}).reset_index()\n",
    "\n",
    "# Merge award counts\n",
    "df_grouped = df_grouped.merge(award_counts, on=\"pi_id\", how=\"left\")\n",
    "print(f\"Grouped DataFrame created with shape: {df_grouped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ab9476",
   "metadata": {},
   "source": [
    "## Feature Normalization\n",
    "\n",
    "Scales the aggregated numeric features (`experience_years`, `award_count`) to a range between 0 and 1 using `MinMaxScaler`. Normalization prevents features with larger ranges from disproportionately influencing distance-based calculations or models sensitive to feature scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1847397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing features...\n",
      "Features normalized.\n"
     ]
    }
   ],
   "source": [
    "# Normalize numeric features\n",
    "print(\"Normalizing features...\")\n",
    "scaler = MinMaxScaler()\n",
    "# Ensure columns exist before scaling\n",
    "features_to_scale = [\"experience_years\", \"award_count\"]\n",
    "if all(col in df_grouped.columns for col in features_to_scale):\n",
    "    df_grouped[[\"exp_norm\", \"award_norm\"]] = scaler.fit_transform(df_grouped[features_to_scale])\n",
    "    print(\"Features normalized.\")\n",
    "else:\n",
    "    print(f\"Warning: One or more columns ({features_to_scale}) not found for normalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b1d16",
   "metadata": {},
   "source": [
    "## Helper and Core Logic Functions\n",
    "\n",
    "This section defines the functions that implement the core logic for finding and ranking influencers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f72ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to get collaborators (as used in format_influencer_data) ---\n",
    "def get_collaborators_for_awards(df: pd.DataFrame, award_titles: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Helper to get collaborators for specific awards.\"\"\"\n",
    "    collaborators = {}\n",
    "    # Optimize by filtering the main df once for relevant roles and titles\n",
    "    relevant_awards_df = df[\n",
    "        df['award_title'].isin(award_titles) &\n",
    "        df['role'].isin(['Principal Investigator', 'Co-Principal Investigator'])\n",
    "    ]\n",
    "    for title in award_titles:\n",
    "        # Filter further for the specific title\n",
    "        award_pis = relevant_awards_df[relevant_awards_df['award_title'] == title]\n",
    "        # Get unique names, handling potential missing names and ensuring they are strings\n",
    "        names = [str(name) for name in award_pis['pi_full_name'].unique() if pd.notna(name)]\n",
    "        collaborators[title] = names\n",
    "    return collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bac3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to format data (as used in the LLM call) ---\n",
    "def format_influencer_data(df: pd.DataFrame, pi_ids: List[str]) -> Tuple[str, Dict[str, str]]:\n",
    "    print(f\"Formatting influencer data for PI IDs: {pi_ids}...\")\n",
    "    formatted_data = \"\"\n",
    "    pi_names = {} # Dictionary to store PI names mapped to IDs\n",
    "\n",
    "    # Filter main df once for all relevant PIs and valid roles\n",
    "    filtered_df = df[\n",
    "        df['pi_id'].isin(pi_ids) &\n",
    "        df['role'].isin(['Principal Investigator', 'Co-Principal Investigator'])\n",
    "    ].copy() # Ensure it's a copy\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"Warning: No data found for any specified PI IDs with roles PI/Co-PI.\")\n",
    "        formatted_data = \"No data could be retrieved for the specified potential influencers with roles PI/Co-PI.\\n\"\n",
    "        for pi_id in pi_ids:\n",
    "             pi_names[pi_id] = f\"PI ID {pi_id}\" # Use ID as placeholder name\n",
    "        return formatted_data, pi_names\n",
    "\n",
    "    # Iterate through the requested PI IDs to structure the output\n",
    "    for pi_id in pi_ids:\n",
    "        pi_specific_data = filtered_df[filtered_df['pi_id'] == pi_id]\n",
    "\n",
    "        if not pi_specific_data.empty:\n",
    "            # Get consistent name from the first entry (handle potential NAs)\n",
    "            full_name = pi_specific_data['pi_full_name'].dropna().iloc[0] if not pi_specific_data['pi_full_name'].dropna().empty else f\"PI ID {pi_id}\"\n",
    "            pi_names[pi_id] = full_name\n",
    "            print(f\"  Processing data for {full_name} ({pi_id})...\")\n",
    "\n",
    "            formatted_data += f\"--- Potential Influencer: {full_name} (ID: {pi_id}) ---\\n\"\n",
    "\n",
    "            # --- Project & Connection Analysis ---\n",
    "            # Count unique *non-null* award titles for this PI\n",
    "            unique_award_titles = pi_specific_data['award_title'].dropna().unique()\n",
    "            num_projects = len(unique_award_titles)\n",
    "            formatted_data += f\"Total Projects Involved In (as PI/Co-PI): {num_projects}\\n\"\n",
    "\n",
    "            # Get all collaborators across these unique projects using the helper function\n",
    "            # We need the full df here to find collaborators on the *same* awards\n",
    "            collaborators_by_award = get_collaborators_for_awards(df, list(unique_award_titles))\n",
    "            all_collaborators = set()\n",
    "            for title, names in collaborators_by_award.items():\n",
    "                # Add collaborators, excluding the PI themselves\n",
    "                all_collaborators.update(name for name in names if name != full_name and pd.notna(name)) # Ensure name is not NA\n",
    "\n",
    "            num_unique_collaborators = len(all_collaborators)\n",
    "            formatted_data += f\"Total Unique Collaborators (excluding self): {num_unique_collaborators}\\n\"\n",
    "            # Optionally list some collaborators:\n",
    "            collaborators_preview = \", \".join(list(all_collaborators)[:5]) # Preview first 5\n",
    "            formatted_data += f\"  Collaborators Sample: {collaborators_preview}{'...' if num_unique_collaborators > 5 else ''}\\n\"\n",
    "\n",
    "            # --- Field Diversity Analysis ---\n",
    "            # Use the PI-specific data for fields\n",
    "            unique_elements = pi_specific_data['program_element'].dropna().unique()\n",
    "            unique_references = pi_specific_data['program_reference'].dropna().unique()\n",
    "            # Combine unique fields into a set\n",
    "            all_fields = set(unique_elements) | set(unique_references)\n",
    "            num_unique_fields = len(all_fields)\n",
    "            formatted_data += f\"Number of Unique Research Fields (Program Elements/References): {num_unique_fields}\\n\"\n",
    "            # Optionally list some fields:\n",
    "            fields_preview = \", \".join(list(all_fields)[:5]) # Preview first 5\n",
    "            formatted_data += f\"  Fields Sample: {fields_preview}{'...' if num_unique_fields > 5 else ''}\\n\\n\"\n",
    "\n",
    "        else:\n",
    "            # Handle case where a specific PI ID from the list had no PI/Co-PI data\n",
    "            formatted_data += f\"--- Potential Influencer ID: {pi_id} ---\\n\"\n",
    "            formatted_data += \"No award data found in the provided dataset for this PI with role PI/Co-PI.\\n\\n\"\n",
    "            pi_names[pi_id] = f\"PI ID {pi_id}\" # Use ID as placeholder name\n",
    "\n",
    "    print(\"Influencer data formatting complete.\")\n",
    "    return formatted_data, pi_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91908b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Prompt Function ---\n",
    "def generate_influencer_prompt(formatted_data_string: str, pi_names_dict: Dict[str, str]) -> str:\n",
    "    print(\"Generating influencer prompt...\")\n",
    "    # Get names from the dictionary values, filtering out placeholders if needed\n",
    "    candidate_names_list = \", \".join(name for name in pi_names_dict.values() if not name.startswith(\"PI ID\"))\n",
    "    if not candidate_names_list:\n",
    "        candidate_names_list = \"the specified PIs\" # Fallback\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Context:\n",
    "        You are an AI assistant analyzing research collaboration data to identify 'influencers'. An influencer is defined as a researcher who has significant connections within the network, demonstrated by:\n",
    "        1.  Being involved (as PI or Co-PI) in a relatively high number of distinct projects/awards.\n",
    "        2.  Having collaborated with a relatively high number of unique individuals (other PIs or Co-PIs on the same projects).\n",
    "        3.  Having experience across a diverse range of research fields (indicated by different Program Elements or Program References associated with their awards).\n",
    "\n",
    "        Below is summarized data for potential influencers ({candidate_names_list}):\n",
    "\n",
    "        {formatted_data_string}\n",
    "\n",
    "        Task:\n",
    "        Based *only* on the summarized information provided above, please analyze each researcher's profile according to the 'influencer' criteria (number of projects, number of unique collaborators, and field diversity count).\n",
    "\n",
    "        Rank these individuals ({candidate_names_list}) from most influential to least influential based *solely* on the metrics presented in the context.\n",
    "\n",
    "        Provide a clear ranking (numbered list) and a concise justification for your ranking. For each person in the ranking, explicitly state the project count, collaborator count, and field count that you used from the provided context to make your decision.\n",
    "    \"\"\"\n",
    "    # Removed the last sentence \"The ranking prioritizes...\" from the prompt to avoid leading the model.\n",
    "    print(\"Influencer prompt generated.\")\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "707ec434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Gemini Response Function ---\n",
    "def get_gemini_response(model: genai.GenerativeModel, prompt: str) -> Tuple[Optional[str], float]:\n",
    "    \"\"\" Sends prompt to Gemini, streams response, measures time. \"\"\"\n",
    "    print(\"--- Sending Request to Gemini ---\")\n",
    "    start_time = time.time()\n",
    "    full_response_text = \"\"\n",
    "    contents = [prompt]\n",
    "\n",
    "    try:\n",
    "        # Use generate_content for non-streaming if streaming causes issues or for simpler handling\n",
    "        response = model.generate_content(contents)\n",
    "        # Check if response has parts and text (structure might vary)\n",
    "        if hasattr(response, 'text'):\n",
    "            full_response_text = response.text\n",
    "        elif hasattr(response, 'parts') and response.parts:\n",
    "             full_response_text = \"\".join(part.text for part in response.parts)\n",
    "        else:\n",
    "             # Fallback or specific handling if the structure is different\n",
    "             # Check the actual response object structure if this happens\n",
    "             print(\"Warning: Unexpected response structure from Gemini.\")\n",
    "             # Try a common alternative structure (may need adjustment)\n",
    "             if hasattr(response, 'candidates') and response.candidates:\n",
    "                 full_response_text = \"\".join(part.text for part in response.candidates[0].content.parts)\n",
    "\n",
    "        response_time = time.time() - start_time\n",
    "        # Simple check if the response seems empty or like an error message\n",
    "        if not full_response_text or \"error\" in full_response_text.lower() or \"could not process\" in full_response_text.lower():\n",
    "             print(f\"\\nWarning: Received potentially empty or error response from Gemini after {response_time:.2f}s.\")\n",
    "             print(f\"Response received: {full_response_text}\")\n",
    "        else:\n",
    "             print(f\"\\nResponse generated successfully in {response_time:.2f} seconds.\")\n",
    "\n",
    "        # print(f\"\\n--- Full Raw Response ---\\n{full_response_text}\\n------------------------\") # Optional: print raw response for debug\n",
    "\n",
    "        return full_response_text, response_time\n",
    "\n",
    "    except AttributeError as e:\n",
    "        response_time = time.time() - start_time\n",
    "        print(f\"\\nError: Attribute error during API call (check model object/API setup): {e}\")\n",
    "        return None, response_time\n",
    "    except Exception as e:\n",
    "        response_time = time.time() - start_time\n",
    "        # More detailed error logging\n",
    "        import traceback\n",
    "        print(f\"\\nAn error occurred during the Gemini API call: {e}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        print(f\"Attempt failed after {response_time:.2f} seconds.\")\n",
    "        return None, response_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5013cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to identify influencers using LLM ---\n",
    "def identify_influencer_llm(df: pd.DataFrame, model: genai.GenerativeModel, pi_ids: List[str]) -> Optional[str]:\n",
    "    print(f\"\\n--- Starting Influencer Identification Process for PI IDs: {pi_ids} ---\")\n",
    "\n",
    "    # 1. Format Data for Influencer Analysis\n",
    "    formatted_text, pi_names = format_influencer_data(df, pi_ids)\n",
    "\n",
    "    # Add a check here: If formatted_text indicates no data, stop.\n",
    "    if \"No data could be retrieved\" in formatted_text or not pi_names or all(name.startswith(\"PI ID\") for name in pi_names.values()):\n",
    "         print(\"Stopping LLM influencer identification: No usable data formatted for the specified PIs.\")\n",
    "         return \"Could not generate influencer ranking: No valid data found for the specified PI IDs with PI/Co-PI roles.\"\n",
    "\n",
    "    # 2. Generate Influencer Prompt\n",
    "    prompt_text = generate_influencer_prompt(formatted_text, pi_names)\n",
    "    # print(\"\\n--- Generated Prompt ---\") # Optional: Debugging\n",
    "    # print(prompt_text) # Optional: Debugging\n",
    "    # print(\"----------------------\") # Optional: Debugging\n",
    "\n",
    "    # 3. Get Response\n",
    "    print(\"--- Sending Request to Gemini for Influencer Ranking ---\")\n",
    "    ranking_result, duration = get_gemini_response(model, prompt_text)\n",
    "\n",
    "    if ranking_result:\n",
    "        print(f\"--- Influencer Identification LLM call complete ({duration:.2f}s) ---\")\n",
    "        return ranking_result # Return the text received from the LLM\n",
    "    else:\n",
    "        print(\"--- Influencer Identification Failed (LLM Error) ---\")\n",
    "        return \"Failed to get influencer ranking from the model due to API error.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10402bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_award = df[df.pi_id == '269779708'].award_title.unique()\n",
    "# len(unique_award), unique_award"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2000bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Select Candidate PIs ---\n",
    "def select_candidate_pis(\n",
    "    df: pd.DataFrame,\n",
    "    df_grouped: pd.DataFrame,\n",
    "    embedder, # Your SentenceTransformer model\n",
    "    criterion_type: str, # \"topic\" or \"department\"\n",
    "    criterion_value: str, # The actual topic or department name\n",
    "    top_k: int = 10 # Number of top candidates to select\n",
    ") -> List[str]:\n",
    "    \"\"\"Selects candidate PI IDs based on topic or department.\"\"\"\n",
    "    print(f\"Selecting top {top_k} candidates based on {criterion_type}: '{criterion_value}'...\")\n",
    "    candidate_ids = []\n",
    "\n",
    "    if criterion_type == \"topic\":\n",
    "        # Check necessary components\n",
    "        if 'text_embedding' not in df_grouped.columns or embedder is None:\n",
    "            print(\"Error: df_grouped['text_embedding'] and embedder required for topic search.\")\n",
    "            return []\n",
    "        if df_grouped['text_embedding'].isnull().any():\n",
    "             print(\"Warning: Null values found in 'text_embedding'. Filling with zeros for similarity calculation.\")\n",
    "             # Ensure embeddings are numpy arrays and handle potential Nones/NaNs before stacking\n",
    "             embeddings_list = [emb if isinstance(emb, np.ndarray) else np.zeros(embedder.get_sentence_embedding_dimension()) for emb in df_grouped['text_embedding']]\n",
    "             all_embeddings = np.stack(embeddings_list)\n",
    "        else:\n",
    "             all_embeddings = np.stack(df_grouped['text_embedding'].values)\n",
    "\n",
    "        if not criterion_value or not isinstance(criterion_value, str):\n",
    "            print(\"Error: Invalid topic criterion value provided.\")\n",
    "            return []\n",
    "\n",
    "        # Compute embedding for the research topic\n",
    "        try:\n",
    "            topic_emb = embedder.encode(criterion_value)\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding topic '{criterion_value}': {e}\")\n",
    "            return []\n",
    "\n",
    "        # Calculate similarity\n",
    "        try:\n",
    "            similarities = cosine_similarity([topic_emb], all_embeddings)[0]\n",
    "            # Get indices of top k PIs sorted by similarity\n",
    "            # Handle cases where k is larger than the number of PIs\n",
    "            num_candidates = min(top_k, len(df_grouped))\n",
    "            top_indices = np.argsort(similarities)[::-1][:num_candidates]\n",
    "            # Get the corresponding PI IDs\n",
    "            candidate_ids = df_grouped.iloc[top_indices]['pi_id'].tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarities or selecting top K: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    elif criterion_type == \"department\":\n",
    "        # Ensure 'department' column exists\n",
    "        if 'department' not in df.columns:\n",
    "             print(\"Error: 'department' column not found in the main DataFrame.\")\n",
    "             return []\n",
    "        if not criterion_value or not isinstance(criterion_value, str):\n",
    "             print(\"Error: Invalid department criterion value provided.\")\n",
    "             return []\n",
    "\n",
    "        # Filter the main df by department (case-insensitive partial match)\n",
    "        try:\n",
    "            # Make matching robust to NaN and case\n",
    "            dept_match_df = df[df['department'].str.contains(criterion_value, case=False, na=False)]\n",
    "        except Exception as e:\n",
    "            print(f\"Error filtering departments: {e}\")\n",
    "            return []\n",
    "\n",
    "        if dept_match_df.empty:\n",
    "            print(f\"No PIs found matching department pattern: '{criterion_value}'\")\n",
    "            return []\n",
    "\n",
    "        # Get unique PI IDs from the matching departments\n",
    "        unique_dept_pi_ids = dept_match_df['pi_id'].unique().tolist() # Convert to list\n",
    "\n",
    "        # If more than top_k PIs, rank them by award count from df_grouped\n",
    "        if len(unique_dept_pi_ids) > top_k:\n",
    "            # Ensure df_grouped and 'award_count' are available\n",
    "            if 'award_count' not in df_grouped.columns:\n",
    "                 print(\"Warning: 'award_count' not in df_grouped. Selecting first K PIs found.\")\n",
    "                 candidate_ids = unique_dept_pi_ids[:top_k]\n",
    "            else:\n",
    "                 # Filter df_grouped for these PIs and sort\n",
    "                 candidate_subset = df_grouped[df_grouped['pi_id'].isin(unique_dept_pi_ids)].copy() # Use .copy()\n",
    "                 # Handle potential NaNs in award_count before sorting\n",
    "                 candidate_subset['award_count'] = candidate_subset['award_count'].fillna(0)\n",
    "                 ranked_candidates = candidate_subset.sort_values(by='award_count', ascending=False)\n",
    "                 candidate_ids = ranked_candidates.head(top_k)['pi_id'].tolist()\n",
    "                 print(f\"  (Found {len(unique_dept_pi_ids)} PIs, selected top {top_k} based on award count)\")\n",
    "        else:\n",
    "            candidate_ids = unique_dept_pi_ids # Already a list\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Invalid criterion_type '{criterion_type}'. Use 'topic' or 'department'.\")\n",
    "        return []\n",
    "\n",
    "    if not candidate_ids:\n",
    "         print(\"No candidate PI IDs were selected.\")\n",
    "    else:\n",
    "         print(f\"Selected candidate PI IDs: {candidate_ids}\")\n",
    "    return candidate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a95449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Orchestrator Function ---\n",
    "def find_influencers_by_criterion(\n",
    "    df: pd.DataFrame,\n",
    "    df_grouped: pd.DataFrame,\n",
    "    embedder,\n",
    "    model: genai.GenerativeModel,\n",
    "    criterion_type: str,\n",
    "    criterion_value: str,\n",
    "    top_k_candidates: int = 10\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Finds influencers based on a criterion using candidate selection and LLM ranking.\"\"\"\n",
    "    print(f\"\\n--- Starting Influencer Search by {criterion_type.capitalize()}: '{criterion_value}' ---\")\n",
    "\n",
    "    # 1. Select Candidate PIs\n",
    "    candidate_pi_ids = select_candidate_pis(\n",
    "        df, df_grouped, embedder, criterion_type, criterion_value, top_k=top_k_candidates\n",
    "    )\n",
    "\n",
    "    if not candidate_pi_ids:\n",
    "        print(\"No candidates found for the specified criterion.\")\n",
    "        # Provide a more informative message depending on the criterion type\n",
    "        if criterion_type == \"department\":\n",
    "             return f\"Could not find potential influencers: No PIs found matching department pattern '{criterion_value}'.\"\n",
    "        elif criterion_type == \"topic\":\n",
    "             return f\"Could not find potential influencers: No PIs found sufficiently matching the topic '{criterion_value}'.\"\n",
    "        else:\n",
    "             return \"Could not find potential influencers: No candidates selected.\"\n",
    "\n",
    "\n",
    "    # 2. Analyze Selected Candidates using LLM\n",
    "    print(f\"\\n--- Analyzing {len(candidate_pi_ids)} Selected Candidate(s) for Influence ---\")\n",
    "    influencer_ranking_result = identify_influencer_llm(df, model, candidate_pi_ids)\n",
    "\n",
    "    return influencer_ranking_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f35cc",
   "metadata": {},
   "source": [
    "## Verification Function\n",
    "#\n",
    "This function recalculates the key metrics (project count, unique collaborator count, field diversity count) for a *single, specified PI* directly from the base `df` DataFrame. Its purpose is to **verify** that the numbers generated by the `format_influencer_data` function (which are fed to the LLM) are correct according to the raw data and the defined logic. This is useful for debugging and ensuring the LLM's ranking is based on accurate inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# +++ NEW Verification Function ++++++++++++++++++++++++++++++++\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "def verify_pi_metrics(df: pd.DataFrame, target_pi_id: str, target_pi_name: str):\n",
    "    \"\"\"\n",
    "    Calculates and prints key influencer metrics for a specific PI directly\n",
    "    from the DataFrame for verification purposes.\n",
    "\n",
    "    Args:\n",
    "        df: The main DataFrame containing all award and PI records.\n",
    "        target_pi_id: The pi_id of the researcher to verify.\n",
    "        target_pi_name: The full name of the researcher to verify (used for excluding self).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Verification Check for {target_pi_name} (ID: {target_pi_id}) ---\")\n",
    "\n",
    "    # Filter data for the specific PI and relevant roles\n",
    "    pi_data = df[\n",
    "        (df['pi_id'] == target_pi_id) &\n",
    "        (df['role'].isin(['Principal Investigator', 'Co-Principal Investigator']))\n",
    "    ].copy() # Use .copy()\n",
    "\n",
    "    if pi_data.empty:\n",
    "        print(f\"No data found for PI {target_pi_name} (ID: {target_pi_id}) with roles PI/Co-PI.\")\n",
    "        print(\"Metrics cannot be verified.\")\n",
    "        return None, None, None # Return None for all metrics\n",
    "\n",
    "    # 1. Calculate Project Count\n",
    "    # Count unique non-null award titles for this PI\n",
    "    unique_projects = pi_data['award_title'].dropna().unique()\n",
    "    project_count = len(unique_projects)\n",
    "    print(f\"  1. Project Count (Unique Awards as PI/Co-PI): {project_count}\")\n",
    "    # print(f\"     Projects: {list(unique_projects)}\") # Optional: list projects\n",
    "\n",
    "    # 2. Calculate Unique Collaborators Count\n",
    "    if project_count > 0:\n",
    "        # Find all people (PIs/Co-PIs) associated with these specific projects\n",
    "        collaborator_df = df[\n",
    "            df['award_title'].isin(unique_projects) &\n",
    "            df['role'].isin(['Principal Investigator', 'Co-Principal Investigator'])\n",
    "        ]\n",
    "        # Get all unique names involved in these projects\n",
    "        all_involved_names = set(collaborator_df['pi_full_name'].dropna())\n",
    "        # Remove the target PI's name to get collaborators\n",
    "        unique_collaborators = all_involved_names - {target_pi_name}\n",
    "        collaborator_count = len(unique_collaborators)\n",
    "        print(f\"  2. Unique Collaborators Count (Excluding Self): {collaborator_count}\")\n",
    "        # print(f\"     Collaborators: {list(unique_collaborators)}\") # Optional: list collaborators\n",
    "    else:\n",
    "        collaborator_count = 0\n",
    "        print(f\"  2. Unique Collaborators Count: {collaborator_count} (No projects found)\")\n",
    "\n",
    "\n",
    "    # 3. Calculate Field Diversity Count\n",
    "    # Combine unique non-null program elements and references from the PI's data\n",
    "    unique_elements = set(pi_data['program_element'].dropna().unique())\n",
    "    unique_references = set(pi_data['program_reference'].dropna().unique())\n",
    "    all_fields = unique_elements | unique_references\n",
    "    field_count = len(all_fields)\n",
    "    print(f\"  3. Field Diversity Count (Unique Program Elements/References): {field_count}\")\n",
    "    # print(f\"     Fields: {list(all_fields)}\") # Optional: list fields\n",
    "\n",
    "    print(\"--- Verification Check Complete ---\")\n",
    "    return project_count, collaborator_count, field_count\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# +++ End Verification Function ++++++++++++++++++++++++++++++++\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Influencer Search by Department: 'Computer Science' ---\n",
      "Selecting top 20 candidates based on department: 'Computer Science'...\n",
      "  (Found 2938 PIs, selected top 20 based on award count)\n",
      "Selected candidate PI IDs: ['269935164', '269779708', '269765937', '000235919', '270031750', '000207040', '270018850', '269779084', '269985475', '269680242', '270018579', '269852294', '000025066', '000265186', '000234696', '269904795', '269746833', '269823209', '269786775', '269749707']\n",
      "\n",
      "--- Analyzing 20 Selected Candidate(s) for Influence ---\n",
      "\n",
      "--- Starting Influencer Identification Process for PI IDs: ['269935164', '269779708', '269765937', '000235919', '270031750', '000207040', '270018850', '269779084', '269985475', '269680242', '270018579', '269852294', '000025066', '000265186', '000234696', '269904795', '269746833', '269823209', '269786775', '269749707'] ---\n",
      "Formatting influencer data for PI IDs: ['269935164', '269779708', '269765937', '000235919', '270031750', '000207040', '270018850', '269779084', '269985475', '269680242', '270018579', '269852294', '000025066', '000265186', '000234696', '269904795', '269746833', '269823209', '269786775', '269749707']...\n",
      "  Processing data for Yanfang   Ye (269935164)...\n",
      "  Processing data for Prasad   Calyam (269779708)...\n",
      "  Processing data for Tiffany M Barnes (269765937)...\n",
      "  Processing data for Sharad   Mehrotra (000235919)...\n",
      "  Processing data for Ferdinando   Fioretto (270031750)...\n",
      "  Processing data for Dhabaleswar K Panda (000207040)...\n",
      "  Processing data for Ravi Netravali (270018850)...\n",
      "  Processing data for Aditya   Akella (269779084)...\n",
      "  Processing data for Jiliang   Tang (269985475)...\n",
      "  Processing data for Mahmut T Kandemir (269680242)...\n",
      "  Processing data for Adriana   Schulz (270018579)...\n",
      "  Processing data for Fan   Wu (269852294)...\n",
      "  Processing data for Sajal K Das (000025066)...\n",
      "  Processing data for Ioannis A Kakadiaris (000265186)...\n",
      "  Processing data for Aidong Zhang (000234696)...\n",
      "  Processing data for Ritu   Arora (269904795)...\n",
      "  Processing data for Xingquan   Zhu (269746833)...\n",
      "  Processing data for Robin R Murphy (269823209)...\n",
      "  Processing data for Lin   Zhong (269786775)...\n",
      "  Processing data for Wenjing   Lou (269749707)...\n",
      "Influencer data formatting complete.\n",
      "Generating influencer prompt...\n",
      "Influencer prompt generated.\n",
      "--- Sending Request to Gemini for Influencer Ranking ---\n",
      "--- Sending Request to Gemini ---\n",
      "\n",
      "Response generated successfully in 39.79 seconds.\n",
      "--- Influencer Identification LLM call complete (39.79s) ---\n",
      "\n",
      "--- LLM Influencer Ranking Result for Department 'Computer Science' ---\n",
      "Based *solely* on the provided metrics for Total Projects, Total Unique Collaborators, and Number of Unique Research Fields, here is the ranking of potential influencers from most to least influential:\n",
      "\n",
      "1.  **Prasad Calyam:** Projects: 14, Collaborators: 25, Fields: 23. Ranks highest or near-highest in all three categories.\n",
      "2.  **Tiffany M Barnes:** Projects: 11, Collaborators: 30, Fields: 14. High collaborator count and strong performance in projects and fields.\n",
      "3.  **Fan Wu:** Projects: 9, Collaborators: 38, Fields: 13. Exceptionally high collaborator count, coupled with solid project and field numbers.\n",
      "4.  **Dhabaleswar K Panda:** Projects: 9, Collaborators: 20, Fields: 15. Strong field diversity and solid collaborator/project counts.\n",
      "5.  **Jiliang Tang:** Projects: 9, Collaborators: 21, Fields: 12. High collaborator count and consistent numbers in projects and fields.\n",
      "6.  **Sharad Mehrotra:** Projects: 10, Collaborators: 18, Fields: 11. Good project and collaborator counts, mid-range field diversity.\n",
      "7.  **Aidong Zhang:** Projects: 9, Collaborators: 20, Fields: 10. High collaborator count, decent projects, but lower field diversity compared to others in this group.\n",
      "8.  **Ioannis A Kakadiaris:** Projects: 9, Collaborators: 17, Fields: 13. Consistent mid-to-high performance across all three metrics.\n",
      "9.  **Mahmut T Kandemir:** Projects: 9, Collaborators: 17, Fields: 12. Similar to Kakadiaris, with slightly fewer fields.\n",
      "10. **Yanfang Ye:** Projects: 13, Collaborators: 11, Fields: 12. Very high project count, but a relatively lower collaborator count compared to others with similar project/field numbers.\n",
      "11. **Adriana Schulz:** Projects: 9, Collaborators: 15, Fields: 11. Consistent mid-range performance across all metrics.\n",
      "12. **Xingquan Zhu:** Projects: 8, Collaborators: 22, Fields: 11. Very high collaborator count, despite lower project numbers than the top group.\n",
      "13. **Wenjing Lou:** Projects: 8, Collaborators: 15, Fields: 10. Moderate collaborator count, with lower project and field numbers compared to higher-ranked individuals.\n",
      "14. **Lin Zhong:** Projects: 8, Collaborators: 13, Fields: 11. Similar project count to Lou, but slightly fewer collaborators and similar fields.\n",
      "15. **Robin R Murphy:** Projects: 8, Collaborators: 9, Fields: 12. Moderate projects and fields, but a relatively low collaborator count.\n",
      "16. **Aditya Akella:** Projects: 8, Collaborators: 13, Fields: 8. Moderate projects and collaborators, but lower field diversity.\n",
      "17. **Sajal K Das:** Projects: 9, Collaborators: 8, Fields: 15. High field diversity and good project count, but a very low collaborator count impacting network influence.\n",
      "18. **Ferdinando Fioretto:** Projects: 7, Collaborators: 6, Fields: 11. Low project and collaborator counts, mid-range field diversity.\n",
      "19. **Ritu Arora:** Projects: 5, Collaborators: 7, Fields: 4. Low numbers across all three metrics.\n",
      "20. **Ravi Netravali:** Projects: 5, Collaborators: 2, Fields: 8. Very low project and collaborator counts, low field diversity.\n",
      "\n",
      "This ranking is determined by considering the balance and magnitude of the three specified metrics: projects, unique collaborators, and unique research fields. Individuals with consistently high numbers across all categories or exceptionally high numbers in one or two categories while being decent in others are ranked higher.\n",
      "\n",
      "<<< Running Verification for Prasad Calyam >>>\n",
      "\n",
      "--- Verification Check for Prasad Calyam (ID: 269779708) ---\n",
      "  1. Project Count (Unique Awards as PI/Co-PI): 14\n",
      "  2. Unique Collaborators Count (Excluding Self): 25\n",
      "  3. Field Diversity Count (Unique Program Elements/References): 23\n",
      "--- Verification Check Complete ---\n",
      "\n",
      "Verification Summary for Prasad Calyam:\n",
      " - Verified Project Count: 14\n",
      " - Verified Collaborator Count: 25\n",
      " - Verified Field Count: 23\n",
      "\n",
      "Compare these numbers with the metrics reported for this PI in the LLM's ranking above.\n",
      "LLM Reported Metrics (from example output): Projects=14, Collaborators=25, Fields=22\n",
      "--------------------------------------------\n",
      "Project Count Match: True\n",
      "Collaborator Count Match: True\n",
      "Field Count Match: False\n",
      "--> Verification FAILED: One or more calculated metrics DO NOT match the example LLM report. Review data/logic.\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "try:\n",
    "    # --- Find Influencers by DEPARTMENT ---\n",
    "    dept_to_search = \"Computer Science\"\n",
    "    dept_ranking_result_text = find_influencers_by_criterion(\n",
    "        df, df_grouped, embedder, model,\n",
    "        criterion_type=\"department\",\n",
    "        criterion_value=dept_to_search,\n",
    "        top_k_candidates=20 # Keep consistent with your example\n",
    "    )\n",
    "\n",
    "    # Print the LLM's ranking result\n",
    "    if dept_ranking_result_text:\n",
    "        print(f\"\\n--- LLM Influencer Ranking Result for Department '{dept_to_search}' ---\")\n",
    "        print(dept_ranking_result_text)\n",
    "    else:\n",
    "        print(f\"\\n--- LLM Influencer Ranking Failed for Department '{dept_to_search}' ---\")\n",
    "\n",
    "\n",
    "    # --- Verification Step for a specific PI from the results ---\n",
    "    # Manually identify the PI ID and Name you want to check from the output/ranking\n",
    "    pi_id_to_verify = '269779708' # Prasad Calyam's ID from your output\n",
    "    pi_name_to_verify = 'Prasad Calyam' # Prasad Calyam's Name from your output\n",
    "\n",
    "    print(f\"\\n<<< Running Verification for {pi_name_to_verify} >>>\")\n",
    "    # Call the verification function\n",
    "    verified_proj, verified_collab, verified_field = verify_pi_metrics(\n",
    "        df, pi_id_to_verify, pi_name_to_verify\n",
    "    )\n",
    "\n",
    "    # Compare with LLM output (manual comparison or parse LLM output if needed)\n",
    "    if verified_proj is not None: # Check if verification ran successfully\n",
    "        print(f\"\\nVerification Summary for {pi_name_to_verify}:\")\n",
    "        print(f\" - Verified Project Count: {verified_proj}\")\n",
    "        print(f\" - Verified Collaborator Count: {verified_collab}\")\n",
    "        print(f\" - Verified Field Count: {verified_field}\")\n",
    "        print(\"\\nCompare these numbers with the metrics reported for this PI in the LLM's ranking above.\")\n",
    "        # Example LLM reported numbers for Prasad Calyam from your output:\n",
    "        print(\"LLM Reported Metrics (from example output): Projects=14, Collaborators=25, Fields=22\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        match_proj = verified_proj == 14\n",
    "        match_collab = verified_collab == 25\n",
    "        match_field = verified_field == 22\n",
    "        print(f\"Project Count Match: {match_proj}\")\n",
    "        print(f\"Collaborator Count Match: {match_collab}\")\n",
    "        print(f\"Field Count Match: {match_field}\")\n",
    "        if match_proj and match_collab and match_field:\n",
    "            print(\"--> Verification PASSED: All calculated metrics match the example LLM report.\")\n",
    "        else:\n",
    "            print(\"--> Verification FAILED: One or more calculated metrics DO NOT match the example LLM report. Review data/logic.\")\n",
    "        print(\"--------------------------------------------\")\n",
    "\n",
    "\n",
    "except NameError as e:\n",
    "     # Added more specific error checking\n",
    "     if 'df' not in locals() and 'df' not in globals():\n",
    "          print(\"Error: DataFrame 'df' is not defined. Data loading might have failed.\")\n",
    "     elif 'df_grouped' not in locals() and 'df_grouped' not in globals():\n",
    "          print(\"Error: Grouped DataFrame 'df_grouped' is not defined. Grouping might have failed.\")\n",
    "     elif 'embedder' not in locals() and 'embedder' not in globals():\n",
    "         print(\"Error: SentenceTransformer 'embedder' is not defined. Model loading might have failed.\")\n",
    "     elif 'model' not in locals() and 'model' not in globals():\n",
    "         print(\"Error: Gemini 'model' is not defined. Model loading or configuration might have failed.\")\n",
    "     else:\n",
    "         print(f\"Error: Required variable not defined. Details: {e}\")\n",
    "     import traceback\n",
    "     traceback.print_exc() # Print traceback for debugging NameErrors\n",
    "\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during the main execution: {e}\")\n",
    "     import traceback\n",
    "     traceback.print_exc() # Print detailed traceback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
